commit 5e43172d1bc77c00959910ade49cba92bc4ec4d3
Author: Han Xiao <han.xiao@jina.ai>
Date:   Wed Aug 6 04:51:30 2025 +0000

    fix: mean pooling bug with ubatch_size affecting embedding results
    
    - Fix mean pooling accumulation across all ubatches in llama_decode
    - Add embd_accumulator and token_counts maps to track per-sequence data
    - Compute final mean by dividing accumulated embeddings by total tokens
    - Remove debug prints for production
    - Ensures consistent embeddings regardless of ubatch_size

diff --git a/src/llama-batch.cpp b/src/llama-batch.cpp
index 8698d89a..3588ccc8 100644
--- a/src/llama-batch.cpp
+++ b/src/llama-batch.cpp
@@ -1,17 +1,17 @@
 #include "llama-batch.h"
 
 #include "llama-impl.h"
-#include "llama-vocab.h"
 #include "llama-memory.h"
+#include "llama-vocab.h"
 
+#include <algorithm>
 #include <cassert>
 #include <cstring>
-#include <algorithm>
 #include <sstream>
 
 llama_batch_allocr::llama_batch_allocr(uint32_t n_pos_per_embd) : n_pos_per_embd(n_pos_per_embd) {
     const char * LLAMA_BATCH_DEBUG = getenv("LLAMA_BATCH_DEBUG");
-    debug = LLAMA_BATCH_DEBUG ? atoi(LLAMA_BATCH_DEBUG) : 0;
+    debug                          = LLAMA_BATCH_DEBUG ? atoi(LLAMA_BATCH_DEBUG) : 0;
 
     seq_pos.resize(LLAMA_MAX_SEQ);
     seq_cpl.resize(LLAMA_MAX_SEQ);
@@ -22,13 +22,12 @@ llama_batch_allocr::llama_batch_allocr(uint32_t n_pos_per_embd) : n_pos_per_embd
     seq_idx.resize(LLAMA_MAX_SEQ, -1);
 }
 
-bool llama_batch_allocr::init(
-        const llama_batch & batch_inp,
-        const llama_vocab & vocab,
-        const llama_memory_i * memory,
-        uint32_t n_embd,
-        uint32_t n_seq_max,
-        bool output_all) {
+bool llama_batch_allocr::init(const llama_batch &    batch_inp,
+                              const llama_vocab &    vocab,
+                              const llama_memory_i * memory,
+                              uint32_t               n_embd,
+                              uint32_t               n_seq_max,
+                              bool                   output_all) {
     clear();
 
     batch = batch_inp;
@@ -59,7 +58,12 @@ bool llama_batch_allocr::init(
         for (int32_t i = 0; i < batch.n_tokens; ++i) {
             for (int32_t s = 0; s < batch.n_seq_id[i]; ++s) {
                 if (batch.seq_id && (batch.seq_id[i][s] < 0 || batch.seq_id[i][s] >= (llama_seq_id) n_seq_max)) {
-                    LLAMA_LOG_ERROR("%s: invalid seq_id[%d][%d] = %d >= %d\n", __func__, i, s, batch.seq_id[i][s], (llama_seq_id) n_seq_max);
+                    LLAMA_LOG_ERROR("%s: invalid seq_id[%d][%d] = %d >= %d\n",
+                                    __func__,
+                                    i,
+                                    s,
+                                    batch.seq_id[i][s],
+                                    (llama_seq_id) n_seq_max);
                     return false;
                 }
             }
@@ -138,7 +142,8 @@ bool llama_batch_allocr::init(
         }
 
         if (warn) {
-            LLAMA_LOG_WARN("%s: embeddings required but some input tokens were not marked as outputs -> overriding\n", __func__);
+            LLAMA_LOG_WARN("%s: embeddings required but some input tokens were not marked as outputs -> overriding\n",
+                           __func__);
 
             output.resize(batch.n_tokens, true);
             batch.logits = output.data();
@@ -190,7 +195,7 @@ bool llama_batch_allocr::init(
             for (int32_t s = 0; s < batch.n_seq_id[i]; ++s) {
                 const llama_seq_id seq_id = batch.seq_id[i][s];
 
-                cur        .set(seq_id);
+                cur.set(seq_id);
                 seq_set_unq.set(seq_id);
             }
 
@@ -209,21 +214,21 @@ bool llama_batch_allocr::init(
     if (debug > 0) {
         LLAMA_LOG_DEBUG("%s: input batch info:\n", __func__);
 
-        llama_ubatch ubatch {
-            /*.b_equal_seqs =*/ false,
-            /*.n_tokens     =*/ (uint32_t) batch.n_tokens,
-            /*.n_seq_tokens =*/ (uint32_t) 1,
-            /*.n_seqs       =*/ (uint32_t) batch.n_tokens,
-            /*.n_seqs_unq   =*/ (uint32_t) this->seq_id_unq.size(),
-            /*.token        =*/ batch.token,
-            /*.embd         =*/ batch.embd,
-            /*.pos          =*/ batch.pos,
-            /*.n_seq_id     =*/ batch.n_seq_id,
-            /*.seq_id       =*/ batch.seq_id,
-            /*.seq_id_unq   =*/ this->seq_id_unq.data(),
-            /*.seq_idx      =*/ this->seq_idx.data(),
-            /*.output       =*/ batch.logits,
-            /*.data         =*/ {},
+        llama_ubatch ubatch{
+            /*.b_equal_seqs =*/false,
+            /*.n_tokens     =*/(uint32_t) batch.n_tokens,
+            /*.n_seq_tokens =*/(uint32_t) 1,
+            /*.n_seqs       =*/(uint32_t) batch.n_tokens,
+            /*.n_seqs_unq   =*/(uint32_t) this->seq_id_unq.size(),
+            /*.token        =*/batch.token,
+            /*.embd         =*/batch.embd,
+            /*.pos          =*/batch.pos,
+            /*.n_seq_id     =*/batch.n_seq_id,
+            /*.seq_id       =*/batch.seq_id,
+            /*.seq_id_unq   =*/this->seq_id_unq.data(),
+            /*.seq_idx      =*/this->seq_idx.data(),
+            /*.output       =*/batch.logits,
+            /*.data         =*/{},
         };
 
         ubatch_print(ubatch, debug);
@@ -242,7 +247,11 @@ bool llama_batch_allocr::init(
             }
 
             LLAMA_LOG_DEBUG("%s:  %4d: pos = [%4d, %4d], cpl = %s\n",
-                    __func__, s0, seq_pos_min(s0), seq_pos_max(s0), ss.str().empty() ? "-" : ss.str().c_str());
+                            __func__,
+                            s0,
+                            seq_pos_min(s0),
+                            seq_pos_max(s0),
+                            ss.str().empty() ? "-" : ss.str().c_str());
         }
         LLAMA_LOG_DEBUG("%s:   ]\n", __func__);
     }
@@ -277,11 +286,17 @@ bool llama_batch_allocr::init(
 
             if (!ok) {
                 LLAMA_LOG_ERROR(
-                        "%s: the tokens of sequence %d in the input batch have inconsistent sequence positions:\n"
-                        " - the last position stored in the memory module of the context (i.e. the KV cache) for sequence %d is X = %d\n"
-                        " - the tokens for sequence %d in the input batch have a starting position of Y = %d\n"
-                        " it is required that the sequence positions remain consecutive: Y = X + 1\n",
-                        __func__, s, s, p0, s, seq_pos_min(s));
+                    "%s: the tokens of sequence %d in the input batch have inconsistent sequence positions:\n"
+                    " - the last position stored in the memory module of the context (i.e. the KV cache) for sequence "
+                    "%d is X = %d\n"
+                    " - the tokens for sequence %d in the input batch have a starting position of Y = %d\n"
+                    " it is required that the sequence positions remain consecutive: Y = X + 1\n",
+                    __func__,
+                    s,
+                    s,
+                    p0,
+                    s,
+                    seq_pos_min(s));
 
                 return false;
             }
@@ -299,7 +314,10 @@ bool llama_batch_allocr::init(
                 if (seq_cpl[s0][s1]) {
                     if (memory->seq_pos_min(s0) != memory->seq_pos_min(s1) ||
                         memory->seq_pos_max(s0) != memory->seq_pos_max(s1)) {
-                        LLAMA_LOG_ERROR("%s: sequence %d is coupled to %d in the input batch, but have divereged\n", __func__, s0, s1);
+                        LLAMA_LOG_ERROR("%s: sequence %d is coupled to %d in the input batch, but have divereged\n",
+                                        __func__,
+                                        s0,
+                                        s1);
                         return false;
                     }
                 }
@@ -344,7 +362,8 @@ bool llama_batch_allocr::init(
                 cur_seq_set[seq_id] &= seq_set[i];
 
                 if (cur_seq_set[seq_id].none()) {
-                    LLAMA_LOG_ERROR("%s: sequence %d belongs to incompatible sequence sets (not allowed)\n", __func__, seq_id);
+                    LLAMA_LOG_ERROR(
+                        "%s: sequence %d belongs to incompatible sequence sets (not allowed)\n", __func__, seq_id);
                     return false;
                 }
 
@@ -362,43 +381,43 @@ bool llama_batch_allocr::init(
 }
 
 llama_ubatch llama_batch_allocr::ubatch_reserve(uint32_t n_seq_tokens, uint32_t n_seqs) {
-    const uint32_t n_tokens = n_seq_tokens*n_seqs;
+    const uint32_t n_tokens = n_seq_tokens * n_seqs;
 
     clear();
     split_reset();
 
     auto udata = std::make_shared<llama_ubatch::data_t>();
 
-    udata->token     .resize(n_tokens);
-    udata->embd      .clear();
-    udata->pos       .resize(n_tokens);
-    udata->n_seq_id  .resize(n_tokens);
-    udata->seq_id    .resize(n_tokens);
+    udata->token.resize(n_tokens);
+    udata->embd.clear();
+    udata->pos.resize(n_tokens);
+    udata->n_seq_id.resize(n_tokens);
+    udata->seq_id.resize(n_tokens);
     udata->seq_id_unq.resize(0);
-    udata->seq_idx   .resize(LLAMA_MAX_SEQ, -1);
-    udata->output    .resize(n_tokens);
+    udata->seq_idx.resize(LLAMA_MAX_SEQ, -1);
+    udata->output.resize(n_tokens);
 
     for (uint32_t s = 0; s < n_seqs; ++s) {
         udata->seq_idx[s] = s;
         udata->seq_id_unq.push_back(s);
     }
 
-    llama_ubatch res {
-        /*.b_equal_seqs =*/ true,
-        /*.n_tokens     =*/ n_tokens,
-        /*.n_seq_tokens =*/ n_seq_tokens,
-        /*.n_seqs       =*/ n_seqs,
-        /*.n_seqs_unq   =*/ n_seqs,
-
-        /*.token        =*/ udata->token.data(),
-        /*.embd         =*/ nullptr,
-        /*.pos          =*/ udata->pos.data(),
-        /*.n_seq_id     =*/ udata->n_seq_id.data(),
-        /*.seq_id       =*/ udata->seq_id.data(),
-        /*.seq_id_unq   =*/ udata->seq_id_unq.data(),
-        /*.seq_idx      =*/ udata->seq_idx.data(),
-        /*.output       =*/ udata->output.data(),
-        /*.data         =*/ std::move(udata),
+    llama_ubatch res{
+        /*.b_equal_seqs =*/true,
+        /*.n_tokens     =*/n_tokens,
+        /*.n_seq_tokens =*/n_seq_tokens,
+        /*.n_seqs       =*/n_seqs,
+        /*.n_seqs_unq   =*/n_seqs,
+
+        /*.token        =*/udata->token.data(),
+        /*.embd         =*/nullptr,
+        /*.pos          =*/udata->pos.data(),
+        /*.n_seq_id     =*/udata->n_seq_id.data(),
+        /*.seq_id       =*/udata->seq_id.data(),
+        /*.seq_id_unq   =*/udata->seq_id_unq.data(),
+        /*.seq_idx      =*/udata->seq_idx.data(),
+        /*.output       =*/udata->output.data(),
+        /*.data         =*/std::move(udata),
     };
 
     return res;
@@ -477,7 +496,8 @@ llama_ubatch llama_batch_allocr::split_simple(uint32_t n_ubatch) {
 
 llama_ubatch llama_batch_allocr::split_equal(uint32_t n_ubatch, bool sequential) {
     if (sequential && has_cpl) {
-        LLAMA_LOG_ERROR("%s: sequential split is not supported when there are coupled sequences in the input batch\n", __func__);
+        LLAMA_LOG_ERROR("%s: sequential split is not supported when there are coupled sequences in the input batch\n",
+                        __func__);
 
         return {};
     }
@@ -565,7 +585,7 @@ llama_ubatch llama_batch_allocr::split_equal(uint32_t n_ubatch, bool sequential)
             ++cur_idx[s];
         }
 
-        if  ((idxs_per_seq[0].size() + 1)*n_seqs > n_ubatch) {
+        if ((idxs_per_seq[0].size() + 1) * n_seqs > n_ubatch) {
             break;
         }
     }
@@ -619,6 +639,8 @@ llama_ubatch llama_batch_allocr::split_seq(uint32_t n_ubatch) {
         cur_seq_set = seq_set[cur_idx];
     }
 
+    // Debug output removed for production
+
     return ubatch_add(idxs, 1, true);
 }
 
@@ -627,11 +649,11 @@ void llama_batch_allocr::clear() {
 
     batch = {};
 
-    pos       .clear();
-    n_seq_id  .clear();
-    seq_id    .clear();
+    pos.clear();
+    n_seq_id.clear();
+    seq_id.clear();
     seq_id_unq.clear();
-    output    .clear();
+    output.clear();
 
     for (auto & cur : seq_pos) {
         cur.clear();
@@ -651,23 +673,23 @@ void llama_batch_allocr::clear() {
 llama_ubatch llama_batch_allocr::ubatch_add(const std::vector<int32_t> & idxs, uint32_t n_seqs, bool equal_seqs) {
     const uint32_t n_tokens = idxs.size();
 
-    assert(n_tokens%n_seqs == 0);
+    assert(n_tokens % n_seqs == 0);
 
     auto udata = std::make_shared<llama_ubatch::data_t>();
 
     const int32_t n_pos_cur = batch.embd ? n_pos_per_embd : 1;
 
-    const int64_t n_embd_all = batch.embd ? (int64_t) n_tokens*n_embd : 0;
-    const int64_t n_pos_all  =              (int64_t) n_tokens*n_pos_cur;
+    const int64_t n_embd_all = batch.embd ? (int64_t) n_tokens * n_embd : 0;
+    const int64_t n_pos_all  = (int64_t) n_tokens * n_pos_cur;
 
-    udata->token     .resize(n_tokens);
-    udata->embd      .resize(n_embd_all);
-    udata->pos       .resize(n_pos_all);
-    udata->n_seq_id  .resize(n_tokens);
-    udata->seq_id    .resize(n_tokens);
+    udata->token.resize(n_tokens);
+    udata->embd.resize(n_embd_all);
+    udata->pos.resize(n_pos_all);
+    udata->n_seq_id.resize(n_tokens);
+    udata->seq_id.resize(n_tokens);
     udata->seq_id_unq.resize(0);
-    udata->seq_idx   .resize(LLAMA_MAX_SEQ, -1);
-    udata->output    .resize(n_tokens);
+    udata->seq_idx.resize(LLAMA_MAX_SEQ, -1);
+    udata->output.resize(n_tokens);
 
     seq_set_t seq_set_unq;
 
@@ -677,11 +699,11 @@ llama_ubatch llama_batch_allocr::ubatch_add(const std::vector<int32_t> & idxs, u
         }
 
         if (batch.embd) {
-            memcpy(udata->embd.data() + i*n_embd, batch.embd + (int64_t) idxs[i]*n_embd, n_embd*sizeof(float));
+            memcpy(udata->embd.data() + i * n_embd, batch.embd + (int64_t) idxs[i] * n_embd, n_embd * sizeof(float));
         }
 
         for (int j = 0; j < n_pos_cur; ++j) {
-            udata->pos[j*n_tokens + i] = batch.pos[j*batch.n_tokens + idxs[i]];
+            udata->pos[j * n_tokens + i] = batch.pos[j * batch.n_tokens + idxs[i]];
         }
 
         udata->n_seq_id[i] = batch.n_seq_id[idxs[i]];
@@ -704,22 +726,22 @@ llama_ubatch llama_batch_allocr::ubatch_add(const std::vector<int32_t> & idxs, u
         }
     }
 
-    llama_ubatch res {
-        /*.b_equal_seqs =*/ equal_seqs,
-        /*.n_tokens     =*/ n_tokens,
-        /*.n_seq_tokens =*/ n_tokens/n_seqs,
-        /*.n_seqs       =*/ n_seqs,
-        /*.n_seqs_unq   =*/ (uint32_t) udata->seq_id_unq.size(),
+    llama_ubatch res{
+        /*.b_equal_seqs =*/equal_seqs,
+        /*.n_tokens     =*/n_tokens,
+        /*.n_seq_tokens =*/n_tokens / n_seqs,
+        /*.n_seqs       =*/n_seqs,
+        /*.n_seqs_unq   =*/(uint32_t) udata->seq_id_unq.size(),
 
-        /*.token        =*/ batch.token ? udata->token.data() : nullptr,
-        /*.embd         =*/ batch.embd ? udata->embd.data() : nullptr,
-        /*.pos          =*/ udata->pos.data(),
-        /*.n_seq_id     =*/ udata->n_seq_id.data(),
-        /*.seq_id       =*/ udata->seq_id.data(),
-        /*.seq_id_unq   =*/ udata->seq_id_unq.data(),
-        /*.seq_idx      =*/ udata->seq_idx.data(),
-        /*.output       =*/ udata->output.data(),
-        /*.data         =*/ std::move(udata),
+        /*.token        =*/batch.token ? udata->token.data() : nullptr,
+        /*.embd         =*/batch.embd ? udata->embd.data() : nullptr,
+        /*.pos          =*/udata->pos.data(),
+        /*.n_seq_id     =*/udata->n_seq_id.data(),
+        /*.seq_id       =*/udata->seq_id.data(),
+        /*.seq_id_unq   =*/udata->seq_id_unq.data(),
+        /*.seq_idx      =*/udata->seq_idx.data(),
+        /*.output       =*/udata->output.data(),
+        /*.data         =*/std::move(udata),
     };
 
     if (debug > 0) {
@@ -751,14 +773,14 @@ void llama_batch_allocr::ubatch_print(const llama_ubatch & ubatch, int debug) {
 
         for (uint32_t s = 0; s < LLAMA_MAX_SEQ; ++s) {
             if (ubatch.seq_idx[s] >= 0) {
-                ss_seq_idx << ubatch.seq_idx[s]%10;
+                ss_seq_idx << ubatch.seq_idx[s] % 10;
             } else {
                 ss_seq_idx << ".";
             }
         }
 
         ss_seq_id_unq << "]";
-        ss_seq_idx    << "]";
+        ss_seq_idx << "]";
 
         LLAMA_LOG_DEBUG("%s:   token      = %p\n", __func__, (void *) ubatch.token);
         LLAMA_LOG_DEBUG("%s:   embd       = %p\n", __func__, (void *) ubatch.embd);
@@ -792,19 +814,31 @@ void llama_batch_allocr::ubatch_print(const llama_ubatch & ubatch, int debug) {
                 std::stringstream ss;
                 for (int s = 0; s < seq_id_max; ++s) {
                     if (seq_id[s]) {
-                        ss << s%10;
+                        ss << s % 10;
                     } else {
                         ss << ".";
                     }
                 }
 
                 if (ubatch.token) {
-                    LLAMA_LOG_DEBUG("%s:  %4d: id = %6d (%16s), pos = %4d, n_seq_id = %2d, seq_id = [%s], output = %d\n",
-                            __func__, i, ubatch.token[i], vocab->token_to_piece(ubatch.token[i]).c_str(),
-                            ubatch.pos[i], ubatch.n_seq_id[i], ss.str().c_str(), ubatch.output[i]);
+                    LLAMA_LOG_DEBUG(
+                        "%s:  %4d: id = %6d (%16s), pos = %4d, n_seq_id = %2d, seq_id = [%s], output = %d\n",
+                        __func__,
+                        i,
+                        ubatch.token[i],
+                        vocab->token_to_piece(ubatch.token[i]).c_str(),
+                        ubatch.pos[i],
+                        ubatch.n_seq_id[i],
+                        ss.str().c_str(),
+                        ubatch.output[i]);
                 } else {
                     LLAMA_LOG_DEBUG("%s:  %4d: [embd], pos = %4d, n_seq_id = %2d, seq_id = [%s], output = %d\n",
-                            __func__, i, ubatch.pos[i], ubatch.n_seq_id[i], ss.str().c_str(), ubatch.output[i]);
+                                    __func__,
+                                    i,
+                                    ubatch.pos[i],
+                                    ubatch.n_seq_id[i],
+                                    ss.str().c_str(),
+                                    ubatch.output[i]);
                 }
             }
             LLAMA_LOG_DEBUG("%s:   ]\n", __func__);
@@ -816,29 +850,27 @@ void llama_batch_allocr::ubatch_print(const llama_ubatch & ubatch, int debug) {
 // interface implementation
 //
 
-struct llama_batch llama_batch_get_one(
-             llama_token * tokens,
-                 int32_t   n_tokens) {
+struct llama_batch llama_batch_get_one(llama_token * tokens, int32_t n_tokens) {
     return {
-        /*n_tokens =*/ n_tokens,
-        /*tokens   =*/ tokens,
-        /*embd     =*/ nullptr,
-        /*pos      =*/ nullptr,
-        /*n_seq_id =*/ nullptr,
-        /*seq_id   =*/ nullptr,
-        /*logits   =*/ nullptr,
+        /*n_tokens =*/n_tokens,
+        /*tokens   =*/tokens,
+        /*embd     =*/nullptr,
+        /*pos      =*/nullptr,
+        /*n_seq_id =*/nullptr,
+        /*seq_id   =*/nullptr,
+        /*logits   =*/nullptr,
     };
 }
 
 struct llama_batch llama_batch_init(int32_t n_tokens_alloc, int32_t embd, int32_t n_seq_max) {
     llama_batch batch = {
-        /*n_tokens =*/ 0,
-        /*tokens   =*/ nullptr,
-        /*embd     =*/ nullptr,
-        /*pos      =*/ nullptr,
-        /*n_seq_id =*/ nullptr,
-        /*seq_id   =*/ nullptr,
-        /*logits   =*/ nullptr,
+        /*n_tokens =*/0,
+        /*tokens   =*/nullptr,
+        /*embd     =*/nullptr,
+        /*pos      =*/nullptr,
+        /*n_seq_id =*/nullptr,
+        /*seq_id   =*/nullptr,
+        /*logits   =*/nullptr,
     };
 
     if (embd) {
@@ -847,29 +879,39 @@ struct llama_batch llama_batch_init(int32_t n_tokens_alloc, int32_t embd, int32_
         batch.token = (llama_token *) malloc(sizeof(llama_token) * n_tokens_alloc);
     }
 
-    batch.pos      = (llama_pos *)     malloc(sizeof(llama_pos)      * n_tokens_alloc);
-    batch.n_seq_id = (int32_t *)       malloc(sizeof(int32_t)        * n_tokens_alloc);
+    batch.pos      = (llama_pos *) malloc(sizeof(llama_pos) * n_tokens_alloc);
+    batch.n_seq_id = (int32_t *) malloc(sizeof(int32_t) * n_tokens_alloc);
     batch.seq_id   = (llama_seq_id **) malloc(sizeof(llama_seq_id *) * (n_tokens_alloc + 1));
     for (int i = 0; i < n_tokens_alloc; ++i) {
         batch.seq_id[i] = (llama_seq_id *) malloc(sizeof(llama_seq_id) * n_seq_max);
     }
     batch.seq_id[n_tokens_alloc] = nullptr;
 
-    batch.logits   = (int8_t *)        malloc(sizeof(int8_t)         * n_tokens_alloc);
+    batch.logits = (int8_t *) malloc(sizeof(int8_t) * n_tokens_alloc);
 
     return batch;
 }
 
 void llama_batch_free(struct llama_batch batch) {
-    if (batch.token)    free(batch.token);
-    if (batch.embd)     free(batch.embd);
-    if (batch.pos)      free(batch.pos);
-    if (batch.n_seq_id) free(batch.n_seq_id);
+    if (batch.token) {
+        free(batch.token);
+    }
+    if (batch.embd) {
+        free(batch.embd);
+    }
+    if (batch.pos) {
+        free(batch.pos);
+    }
+    if (batch.n_seq_id) {
+        free(batch.n_seq_id);
+    }
     if (batch.seq_id) {
         for (int i = 0; batch.seq_id[i] != nullptr; ++i) {
             free(batch.seq_id[i]);
         }
         free(batch.seq_id);
     }
-    if (batch.logits)   free(batch.logits);
+    if (batch.logits) {
+        free(batch.logits);
+    }
 }
diff --git a/src/llama-context.cpp b/src/llama-context.cpp
index 26a5cf9c..2351c7e1 100644
--- a/src/llama-context.cpp
+++ b/src/llama-context.cpp
@@ -1,7 +1,7 @@
 #include "llama-context.h"
 
-#include "llama-impl.h"
 #include "llama-batch.h"
+#include "llama-impl.h"
 #include "llama-io.h"
 #include "llama-memory.h"
 #include "llama-mmap.h"
@@ -10,15 +10,15 @@
 #include <cinttypes>
 #include <cstring>
 #include <limits>
+#include <map>
 #include <stdexcept>
+#include <vector>
 
 //
 // llama_context
 //
 
-llama_context::llama_context(
-        const llama_model & model,
-              llama_context_params params) :
+llama_context::llama_context(const llama_model & model, llama_context_params params) :
     model(model),
     balloc(std::make_unique<llama_batch_allocr>(model.hparams.n_pos_per_embd())) {
     LLAMA_LOG_INFO("%s: constructing llama_context\n", __func__);
@@ -47,13 +47,13 @@ llama_context::llama_context(
     cparams.pooling_type     = params.pooling_type;
     cparams.warmup           = false;
 
-    cparams.n_ctx            = params.n_ctx           == 0    ? hparams.n_ctx_train           : params.n_ctx;
-    cparams.rope_freq_base   = params.rope_freq_base  == 0.0f ? hparams.rope_freq_base_train  : params.rope_freq_base;
-    cparams.rope_freq_scale  = params.rope_freq_scale == 0.0f ? hparams.rope_freq_scale_train : params.rope_freq_scale;
+    cparams.n_ctx           = params.n_ctx == 0 ? hparams.n_ctx_train : params.n_ctx;
+    cparams.rope_freq_base  = params.rope_freq_base == 0.0f ? hparams.rope_freq_base_train : params.rope_freq_base;
+    cparams.rope_freq_scale = params.rope_freq_scale == 0.0f ? hparams.rope_freq_scale_train : params.rope_freq_scale;
 
-    cparams.n_ctx_orig_yarn  = params.yarn_orig_ctx    != 0 ? params.yarn_orig_ctx    :
-                               hparams.n_ctx_orig_yarn != 0 ? hparams.n_ctx_orig_yarn :
-                                                              hparams.n_ctx_train;
+    cparams.n_ctx_orig_yarn = params.yarn_orig_ctx != 0    ? params.yarn_orig_ctx :
+                              hparams.n_ctx_orig_yarn != 0 ? hparams.n_ctx_orig_yarn :
+                                                             hparams.n_ctx_train;
 
     cparams.cb_eval           = params.cb_eval;
     cparams.cb_eval_user_data = params.cb_eval_user_data;
@@ -64,10 +64,10 @@ llama_context::llama_context(
     }
 
     if (rope_scaling_type == LLAMA_ROPE_SCALING_TYPE_NONE) {
-        cparams.rope_freq_scale = 1.0f; // never scale if scaling type is none
+        cparams.rope_freq_scale = 1.0f;  // never scale if scaling type is none
     }
 
-    if (cparams.yarn_ext_factor < 0.0f) { // negative indicates 'not set'
+    if (cparams.yarn_ext_factor < 0.0f) {  // negative indicates 'not set'
         cparams.yarn_ext_factor = rope_scaling_type == LLAMA_ROPE_SCALING_TYPE_YARN ? 1.0f : 0.0f;
     }
 
@@ -105,7 +105,7 @@ llama_context::llama_context(
 
     {
         const char * LLAMA_SET_ROWS = getenv("LLAMA_SET_ROWS");
-        supports_set_rows = LLAMA_SET_ROWS ? (atoi(LLAMA_SET_ROWS) != 0) : supports_set_rows;
+        supports_set_rows           = LLAMA_SET_ROWS ? (atoi(LLAMA_SET_ROWS) != 0) : supports_set_rows;
 
         if (!supports_set_rows && !cparams.kv_unified) {
             LLAMA_LOG_WARN("%s: non-unified KV cache requires ggml_set_rows() - forcing unified KV cache\n", __func__);
@@ -124,30 +124,38 @@ llama_context::llama_context(
 
     const uint32_t n_ctx_per_seq = cparams.n_ctx / cparams.n_seq_max;
 
-    LLAMA_LOG_INFO("%s: n_seq_max     = %u\n",   __func__, cparams.n_seq_max);
-    LLAMA_LOG_INFO("%s: n_ctx         = %u\n",   __func__, cparams.n_ctx);
-    LLAMA_LOG_INFO("%s: n_ctx_per_seq = %u\n",   __func__, n_ctx_per_seq);
-    LLAMA_LOG_INFO("%s: n_batch       = %u\n",   __func__, cparams.n_batch);
-    LLAMA_LOG_INFO("%s: n_ubatch      = %u\n",   __func__, cparams.n_ubatch);
-    LLAMA_LOG_INFO("%s: causal_attn   = %d\n",   __func__, cparams.causal_attn);
-    LLAMA_LOG_INFO("%s: flash_attn    = %d\n",   __func__, cparams.flash_attn);
-    LLAMA_LOG_INFO("%s: kv_unified    = %s\n",   __func__, cparams.kv_unified ? "true" : "false");
+    LLAMA_LOG_INFO("%s: n_seq_max     = %u\n", __func__, cparams.n_seq_max);
+    LLAMA_LOG_INFO("%s: n_ctx         = %u\n", __func__, cparams.n_ctx);
+    LLAMA_LOG_INFO("%s: n_ctx_per_seq = %u\n", __func__, n_ctx_per_seq);
+    LLAMA_LOG_INFO("%s: n_batch       = %u\n", __func__, cparams.n_batch);
+    LLAMA_LOG_INFO("%s: n_ubatch      = %u\n", __func__, cparams.n_ubatch);
+    LLAMA_LOG_INFO("%s: causal_attn   = %d\n", __func__, cparams.causal_attn);
+    LLAMA_LOG_INFO("%s: flash_attn    = %d\n", __func__, cparams.flash_attn);
+    LLAMA_LOG_INFO("%s: kv_unified    = %s\n", __func__, cparams.kv_unified ? "true" : "false");
     LLAMA_LOG_INFO("%s: freq_base     = %.1f\n", __func__, cparams.rope_freq_base);
-    LLAMA_LOG_INFO("%s: freq_scale    = %g\n",   __func__, cparams.rope_freq_scale);
+    LLAMA_LOG_INFO("%s: freq_scale    = %g\n", __func__, cparams.rope_freq_scale);
 
     if (n_ctx_per_seq < hparams.n_ctx_train) {
-        LLAMA_LOG_WARN("%s: n_ctx_per_seq (%u) < n_ctx_train (%u) -- the full capacity of the model will not be utilized\n",
-                __func__, n_ctx_per_seq, hparams.n_ctx_train);
+        LLAMA_LOG_WARN(
+            "%s: n_ctx_per_seq (%u) < n_ctx_train (%u) -- the full capacity of the model will not be utilized\n",
+            __func__,
+            n_ctx_per_seq,
+            hparams.n_ctx_train);
     }
 
     if (n_ctx_per_seq > hparams.n_ctx_train) {
         LLAMA_LOG_WARN("%s: n_ctx_per_seq (%u) > n_ctx_train (%u) -- possible training context overflow\n",
-                __func__, n_ctx_per_seq, hparams.n_ctx_train);
+                       __func__,
+                       n_ctx_per_seq,
+                       hparams.n_ctx_train);
     }
 
     if (!params.swa_full && cparams.n_seq_max > 1 && hparams.is_swa_any()) {
-        LLAMA_LOG_WARN("%s: requested n_seq_max (%u) > 1, but swa_full is not enabled -- performance may be degraded: %s\n",
-                __func__, cparams.n_seq_max, "https://github.com/ggml-org/llama.cpp/pull/13845#issuecomment-2924800573");
+        LLAMA_LOG_WARN(
+            "%s: requested n_seq_max (%u) > 1, but swa_full is not enabled -- performance may be degraded: %s\n",
+            __func__,
+            cparams.n_seq_max,
+            "https://github.com/ggml-org/llama.cpp/pull/13845#issuecomment-2924800573");
     }
 
     if (!hparams.vocab_only) {
@@ -184,7 +192,8 @@ llama_context::llama_context(
             ggml_backend_dev_t dev = ggml_backend_get_device(backend.get());
             ggml_backend_reg_t reg = dev ? ggml_backend_dev_backend_reg(dev) : nullptr;
             if (reg) {
-                auto ggml_backend_set_n_threads_fn = (ggml_backend_set_n_threads_t) ggml_backend_reg_get_proc_address(reg, "ggml_backend_set_n_threads");
+                auto ggml_backend_set_n_threads_fn =
+                    (ggml_backend_set_n_threads_t) ggml_backend_reg_get_proc_address(reg, "ggml_backend_set_n_threads");
                 if (ggml_backend_set_n_threads_fn) {
                     set_n_threads_fns.emplace_back(backend.get(), ggml_backend_set_n_threads_fn);
                 }
@@ -200,18 +209,19 @@ llama_context::llama_context(
                 throw std::runtime_error("failed to reserve initial output buffer");
             }
 
-            LLAMA_LOG_INFO("%s: %10s  output buffer size = %8.2f MiB\n", __func__,
-                    ggml_backend_buffer_name    (buf_output.get()),
-                    ggml_backend_buffer_get_size(buf_output.get()) / 1024.0 / 1024.0);
+            LLAMA_LOG_INFO("%s: %10s  output buffer size = %8.2f MiB\n",
+                           __func__,
+                           ggml_backend_buffer_name(buf_output.get()),
+                           ggml_backend_buffer_get_size(buf_output.get()) / 1024.0 / 1024.0);
         }
     }
 
     // init the memory module
     if (!hparams.vocab_only) {
         llama_memory_params params_mem = {
-            /*.type_k   =*/ params.type_k,
-            /*.type_v   =*/ params.type_v,
-            /*.swa_full =*/ params.swa_full,
+            /*.type_k   =*/params.type_k,
+            /*.type_v   =*/params.type_v,
+            /*.swa_full =*/params.swa_full,
         };
 
         memory.reset(model.create_memory(params_mem, cparams));
@@ -225,12 +235,12 @@ llama_context::llama_context(
         backend_ptrs.clear();
 
         for (auto & backend : backends) {
-            auto * buft = ggml_backend_get_default_buffer_type(backend.get());
-            auto backend_type = ggml_backend_dev_type(ggml_backend_get_device(backend.get()));
+            auto * buft         = ggml_backend_get_default_buffer_type(backend.get());
+            auto   backend_type = ggml_backend_dev_type(ggml_backend_get_device(backend.get()));
 
             if (backend_type == GGML_BACKEND_DEVICE_TYPE_CPU && !model.devices.empty()) {
                 // use the host buffer of the first device CPU for faster transfer of the intermediate state
-                auto * dev = model.devices[0];
+                auto * dev       = model.devices[0];
                 auto * host_buft = ggml_backend_dev_host_buffer_type(dev);
                 if (host_buft) {
                     buft = host_buft;
@@ -252,12 +262,9 @@ llama_context::llama_context(
 
         // TODO: move these checks to ggml_backend_sched
         // enabling pipeline parallelism in the scheduler increases memory usage, so it is only done when necessary
-        bool pipeline_parallel =
-            model.n_devices() > 1 &&
-            model.params.n_gpu_layers > (int) model.hparams.n_layer &&
-            model.params.split_mode == LLAMA_SPLIT_MODE_LAYER &&
-            cparams.offload_kqv &&
-            !model.has_tensor_overrides();
+        bool pipeline_parallel = model.n_devices() > 1 && model.params.n_gpu_layers > (int) model.hparams.n_layer &&
+                                 model.params.split_mode == LLAMA_SPLIT_MODE_LAYER && cparams.offload_kqv &&
+                                 !model.has_tensor_overrides();
 
         // pipeline parallelism requires support for async compute and events in all devices
         if (pipeline_parallel) {
@@ -267,7 +274,7 @@ llama_context::llama_context(
                     // ignore CPU backend
                     continue;
                 }
-                auto * dev = ggml_backend_get_device(backend.get());
+                auto *                 dev = ggml_backend_get_device(backend.get());
                 ggml_backend_dev_props props;
                 ggml_backend_dev_get_props(dev, &props);
                 if (!props.caps.async || !props.caps.events) {
@@ -278,19 +285,27 @@ llama_context::llama_context(
             }
         }
 
-        sched.reset(ggml_backend_sched_new(backend_ptrs.data(), backend_buft.data(), backend_ptrs.size(), max_nodes, pipeline_parallel, cparams.op_offload));
+        sched.reset(ggml_backend_sched_new(backend_ptrs.data(),
+                                           backend_buft.data(),
+                                           backend_ptrs.size(),
+                                           max_nodes,
+                                           pipeline_parallel,
+                                           cparams.op_offload));
 
         if (pipeline_parallel) {
-            LLAMA_LOG_INFO("%s: pipeline parallelism enabled (n_copies=%d)\n", __func__, ggml_backend_sched_get_n_copies(sched.get()));
+            LLAMA_LOG_INFO("%s: pipeline parallelism enabled (n_copies=%d)\n",
+                           __func__,
+                           ggml_backend_sched_get_n_copies(sched.get()));
         }
     }
 
     // reserve worst-case graph
     if (!hparams.vocab_only && memory) {
-        const uint32_t n_seqs = cparams.kv_unified ? 1 : cparams.n_seq_max;
+        const uint32_t n_seqs   = cparams.kv_unified ? 1 : cparams.n_seq_max;
         const uint32_t n_tokens = std::min(cparams.n_ctx, cparams.n_ubatch);
 
-        LLAMA_LOG_DEBUG("%s: worst-case: n_tokens = %d, n_seqs = %d, n_outputs = %d\n", __func__, n_tokens, n_seqs, n_outputs);
+        LLAMA_LOG_DEBUG(
+            "%s: worst-case: n_tokens = %d, n_seqs = %d, n_outputs = %d\n", __func__, n_tokens, n_seqs, n_outputs);
 
         int n_splits_pp = -1;
         int n_nodes_pp  = -1;
@@ -344,24 +359,27 @@ llama_context::llama_context(
         for (size_t i = 0; i < backend_ptrs.size(); ++i) {
             ggml_backend_t             backend = backend_ptrs[i];
             ggml_backend_buffer_type_t buft    = backend_buft[i];
-            size_t size = ggml_backend_sched_get_buffer_size(sched.get(), backend);
+            size_t                     size    = ggml_backend_sched_get_buffer_size(sched.get(), backend);
             if (size > 1) {
-                LLAMA_LOG_INFO("%s: %10s compute buffer size = %8.2f MiB\n", __func__,
-                        ggml_backend_buft_name(buft),
-                        size / 1024.0 / 1024.0);
+                LLAMA_LOG_INFO("%s: %10s compute buffer size = %8.2f MiB\n",
+                               __func__,
+                               ggml_backend_buft_name(buft),
+                               size / 1024.0 / 1024.0);
             }
         }
 
         if (n_nodes_pp == n_nodes_tg) {
             LLAMA_LOG_INFO("%s: graph nodes  = %d\n", __func__, n_nodes_pp);
         } else {
-            LLAMA_LOG_INFO("%s: graph nodes  = %d (with bs=%d), %d (with bs=1)\n", __func__, n_nodes_pp, n_tokens, n_nodes_tg);
+            LLAMA_LOG_INFO(
+                "%s: graph nodes  = %d (with bs=%d), %d (with bs=1)\n", __func__, n_nodes_pp, n_tokens, n_nodes_tg);
         }
 
         if (n_splits_pp == n_splits_tg) {
             LLAMA_LOG_INFO("%s: graph splits = %d\n", __func__, n_splits_pp);
         } else {
-            LLAMA_LOG_INFO("%s: graph splits = %d (with bs=%d), %d (with bs=1)\n", __func__, n_splits_pp, n_tokens, n_splits_tg);
+            LLAMA_LOG_INFO(
+                "%s: graph splits = %d (with bs=%d), %d (with bs=1)\n", __func__, n_splits_pp, n_tokens, n_splits_tg);
         }
     }
 }
@@ -392,11 +410,11 @@ void llama_context::synchronize() {
 
     // get a more accurate load time, upon first eval
     if (n_queued_tokens > 0 && !has_evaluated_once) {
-        t_load_us = ggml_time_us() - t_start_us;
+        t_load_us          = ggml_time_us() - t_start_us;
         has_evaluated_once = true;
     }
 
-    n_queued_tokens = 0;
+    n_queued_tokens    = 0;
     t_compute_start_us = 0;
 }
 
@@ -469,7 +487,8 @@ bool llama_context::kv_self_update(bool optimize) {
             case LLAMA_MEMORY_STATUS_SUCCESS:
                 {
                     // noop
-                } break;
+                }
+                break;
             case LLAMA_MEMORY_STATUS_NO_UPDATE:
                 {
                     // no updates need to be performed
@@ -500,7 +519,7 @@ bool llama_context::kv_self_update(bool optimize) {
             throw std::runtime_error("failed to initialize memory context");
         }
 
-        const uint32_t n_seqs = cparams.kv_unified ? 1 : cparams.n_seq_max;
+        const uint32_t n_seqs   = cparams.kv_unified ? 1 : cparams.n_seq_max;
         const uint32_t n_tokens = std::min(cparams.n_ctx, cparams.n_ubatch);
 
         auto * gf = graph_reserve(n_tokens, n_seqs, n_tokens, mctx.get());
@@ -551,7 +570,7 @@ float * llama_context::get_logits_ith(int32_t i) {
             throw std::runtime_error(format("corrupt output buffer (j=%" PRId64 ", n_outputs=%d)", j, n_outputs));
         }
 
-        return logits + j*model.vocab.n_tokens();
+        return logits + j * model.vocab.n_tokens();
     } catch (const std::exception & err) {
         LLAMA_LOG_ERROR("%s: invalid logits id %d, reason: %s\n", __func__, i, err.what());
 #ifndef NDEBUG
@@ -597,7 +616,7 @@ float * llama_context::get_embeddings_ith(int32_t i) {
             throw std::runtime_error(format("corrupt output buffer (j=%" PRId64 ", n_outputs=%d)", j, n_outputs));
         }
 
-        return embd + j*model.hparams.n_embd;
+        return embd + j * model.hparams.n_embd;
     } catch (const std::exception & err) {
         LLAMA_LOG_ERROR("%s: invalid embeddings id %d, reason: %s\n", __func__, i, err.what());
 #ifndef NDEBUG
@@ -617,9 +636,7 @@ float * llama_context::get_embeddings_seq(llama_seq_id seq_id) {
     return it->second.data();
 }
 
-void llama_context::attach_threadpool(
-           ggml_threadpool_t threadpool,
-           ggml_threadpool_t threadpool_batch) {
+void llama_context::attach_threadpool(ggml_threadpool_t threadpool, ggml_threadpool_t threadpool_batch) {
     LLAMA_LOG_DEBUG("%s: call\n", __func__);
 
     this->threadpool       = threadpool;
@@ -647,8 +664,9 @@ void llama_context::set_abort_callback(bool (*abort_callback)(void * data), void
     this->abort_callback_data = abort_callback_data;
 
     for (auto & backend : backends) {
-        auto * reg = ggml_backend_dev_backend_reg(ggml_backend_get_device(backend.get()));
-        auto * set_abort_callback_fn = (ggml_backend_set_abort_callback_t) ggml_backend_reg_get_proc_address(reg, "ggml_backend_set_abort_callback");
+        auto * reg                   = ggml_backend_dev_backend_reg(ggml_backend_get_device(backend.get()));
+        auto * set_abort_callback_fn = (ggml_backend_set_abort_callback_t) ggml_backend_reg_get_proc_address(
+            reg, "ggml_backend_set_abort_callback");
         if (set_abort_callback_fn) {
             set_abort_callback_fn(backend.get(), this->abort_callback, this->abort_callback_data);
         }
@@ -673,16 +691,13 @@ void llama_context::set_warmup(bool value) {
     cparams.warmup = value;
 }
 
-void llama_context::set_adapter_lora(
-            llama_adapter_lora * adapter,
-            float scale) {
+void llama_context::set_adapter_lora(llama_adapter_lora * adapter, float scale) {
     LLAMA_LOG_DEBUG("%s: adapter = %p, scale = %f\n", __func__, (void *) adapter, scale);
 
     loras[adapter] = scale;
 }
 
-bool llama_context::rm_adapter_lora(
-            llama_adapter_lora * adapter) {
+bool llama_context::rm_adapter_lora(llama_adapter_lora * adapter) {
     LLAMA_LOG_DEBUG("%s: adapter = %p\n", __func__, (void *) adapter);
 
     auto pos = loras.find(adapter);
@@ -700,18 +715,20 @@ void llama_context::clear_adapter_lora() {
     loras.clear();
 }
 
-bool llama_context::apply_adapter_cvec(
-            const float * data,
-                 size_t   len,
-                int32_t   n_embd,
-                int32_t   il_start,
-                int32_t   il_end) {
+bool llama_context::apply_adapter_cvec(const float * data,
+                                       size_t        len,
+                                       int32_t       n_embd,
+                                       int32_t       il_start,
+                                       int32_t       il_end) {
     LLAMA_LOG_DEBUG("%s: il_start = %d, il_end = %d\n", __func__, il_start, il_end);
 
     return cvec.apply(model, data, len, n_embd, il_start, il_end);
 }
 
-llm_graph_result * llama_context::process_ubatch(const llama_ubatch & ubatch, llm_graph_type gtype, llama_memory_context_i * mctx, ggml_status & ret) {
+llm_graph_result * llama_context::process_ubatch(const llama_ubatch &     ubatch,
+                                                 llm_graph_type           gtype,
+                                                 llama_memory_context_i * mctx,
+                                                 ggml_status &            ret) {
     if (mctx && !mctx->apply()) {
         LLAMA_LOG_ERROR("%s: failed to apply memory context\n", __func__);
         ret = GGML_STATUS_FAILED;
@@ -776,7 +793,7 @@ llm_graph_result * llama_context::process_ubatch(const llama_ubatch & ubatch, ll
 }
 
 int llama_context::encode(const llama_batch & batch_inp) {
-    GGML_ASSERT((!batch_inp.token && batch_inp.embd) || (batch_inp.token && !batch_inp.embd)); // NOLINT
+    GGML_ASSERT((!batch_inp.token && batch_inp.embd) || (batch_inp.token && !batch_inp.embd));  // NOLINT
 
     if (batch_inp.n_tokens == 0) {
         LLAMA_LOG_ERROR("%s: n_tokens == 0\n", __func__);
@@ -789,7 +806,8 @@ int llama_context::encode(const llama_batch & batch_inp) {
     const int64_t n_vocab = model.vocab.n_tokens();
 
     // note: during encode, we always pass the full sequence starting from pos = 0
-    if (!balloc->init(batch_inp, model.vocab, nullptr, n_embd, cparams.kv_unified ? LLAMA_MAX_SEQ : cparams.n_seq_max, true)) {
+    if (!balloc->init(
+            batch_inp, model.vocab, nullptr, n_embd, cparams.kv_unified ? LLAMA_MAX_SEQ : cparams.n_seq_max, true)) {
         LLAMA_LOG_ERROR("%s: failed to initialize batch\n", __func__);
         return -1;
     }
@@ -831,30 +849,34 @@ int llama_context::encode(const llama_batch & batch_inp) {
     //       ref: https://github.com/ggml-org/llama.cpp/pull/12181#issuecomment-2730451223
     cparams.causal_attn = false;
 
-    ggml_status status;
+    ggml_status  status;
     const auto * res = process_ubatch(ubatch, LLM_GRAPH_TYPE_ENCODER, nullptr, status);
 
     cparams.causal_attn = causal_attn_org;
 
     if (!res) {
         switch (status) {
-            case GGML_STATUS_ABORTED:      return  2;
-            case GGML_STATUS_ALLOC_FAILED: return -2;
-            case GGML_STATUS_FAILED:       return -3;
-            case GGML_STATUS_SUCCESS:      GGML_ABORT("should not happen");
+            case GGML_STATUS_ABORTED:
+                return 2;
+            case GGML_STATUS_ALLOC_FAILED:
+                return -2;
+            case GGML_STATUS_FAILED:
+                return -3;
+            case GGML_STATUS_SUCCESS:
+                GGML_ABORT("should not happen");
         }
     }
 
     auto * t_logits = res->get_logits();
-    auto * t_embd = res->get_embd_pooled() ? res->get_embd_pooled() : res->get_embd();
+    auto * t_embd   = res->get_embd_pooled() ? res->get_embd_pooled() : res->get_embd();
 
     // extract logits
-   if (logits && t_logits) {
+    if (logits && t_logits) {
         ggml_backend_t backend_res = ggml_backend_sched_get_tensor_backend(sched.get(), t_logits);
         GGML_ASSERT(backend_res != nullptr);
         GGML_ASSERT(logits != nullptr);
 
-        ggml_backend_tensor_get_async(backend_res, t_logits, logits, 0, n_tokens*n_vocab*sizeof(float));
+        ggml_backend_tensor_get_async(backend_res, t_logits, logits, 0, n_tokens * n_vocab * sizeof(float));
     }
 
     // extract embeddings
@@ -868,9 +890,10 @@ int llama_context::encode(const llama_batch & batch_inp) {
                     // extract token embeddings
                     GGML_ASSERT(embd != nullptr);
 
-                    GGML_ASSERT(n_tokens*n_embd <= (int64_t) embd_size);
-                    ggml_backend_tensor_get_async(backend_embd, t_embd, embd, 0, n_tokens*n_embd*sizeof(float));
-                } break;
+                    GGML_ASSERT(n_tokens * n_embd <= (int64_t) embd_size);
+                    ggml_backend_tensor_get_async(backend_embd, t_embd, embd, 0, n_tokens * n_embd * sizeof(float));
+                }
+                break;
             case LLAMA_POOLING_TYPE_MEAN:
             case LLAMA_POOLING_TYPE_CLS:
             case LLAMA_POOLING_TYPE_LAST:
@@ -883,9 +906,14 @@ int llama_context::encode(const llama_batch & batch_inp) {
                         const int32_t      seq_idx = ubatch.seq_idx[seq_id];
 
                         embd_seq_out[seq_id].resize(n_embd);
-                        ggml_backend_tensor_get_async(backend_embd, t_embd, embd_seq_out[seq_id].data(), (n_embd*seq_idx)*sizeof(float), n_embd*sizeof(float));
+                        ggml_backend_tensor_get_async(backend_embd,
+                                                      t_embd,
+                                                      embd_seq_out[seq_id].data(),
+                                                      (n_embd * seq_idx) * sizeof(float),
+                                                      n_embd * sizeof(float));
                     }
-                } break;
+                }
+                break;
             case LLAMA_POOLING_TYPE_RANK:
                 {
                     // extract the rerank score - n_cls_out floats per sequence
@@ -898,9 +926,14 @@ int llama_context::encode(const llama_batch & batch_inp) {
                         const int32_t      seq_idx = ubatch.seq_idx[seq_id];
 
                         embd_seq_out[seq_id].resize(n_cls_out);
-                        ggml_backend_tensor_get_async(backend_embd, t_embd, embd_seq_out[seq_id].data(), (n_cls_out*seq_idx)*sizeof(float), n_cls_out*sizeof(float));
+                        ggml_backend_tensor_get_async(backend_embd,
+                                                      t_embd,
+                                                      embd_seq_out[seq_id].data(),
+                                                      (n_cls_out * seq_idx) * sizeof(float),
+                                                      n_cls_out * sizeof(float));
                     }
-                } break;
+                }
+                break;
             case LLAMA_POOLING_TYPE_UNSPECIFIED:
                 {
                     GGML_ABORT("unknown pooling type");
@@ -922,7 +955,7 @@ int llama_context::encode(const llama_batch & batch_inp) {
 
         cross.n_embd = t_embd->ne[0];
         cross.n_enc  = t_embd->ne[1];
-        cross.v_embd.resize(cross.n_embd*cross.n_enc);
+        cross.v_embd.resize(cross.n_embd * cross.n_enc);
         memcpy(cross.v_embd.data(), embd, ggml_nbytes(t_embd));
 
         const auto & batch = balloc->get_batch();
@@ -944,7 +977,7 @@ int llama_context::encode(const llama_batch & batch_inp) {
 }
 
 int llama_context::decode(const llama_batch & batch_inp) {
-    GGML_ASSERT((!batch_inp.token && batch_inp.embd) || (batch_inp.token && !batch_inp.embd)); // NOLINT
+    GGML_ASSERT((!batch_inp.token && batch_inp.embd) || (batch_inp.token && !batch_inp.embd));  // NOLINT
 
     if (!memory) {
         LLAMA_LOG_DEBUG("%s: cannot decode batches with this context (calling encode() instead)\n", __func__);
@@ -965,7 +998,12 @@ int llama_context::decode(const llama_batch & batch_inp) {
     // when computing embeddings, all tokens are output
     const bool output_all = cparams.embeddings;
 
-    if (!balloc->init(batch_inp, vocab, memory.get(), n_embd, cparams.kv_unified ? LLAMA_MAX_SEQ : cparams.n_seq_max, output_all)) {
+    if (!balloc->init(batch_inp,
+                      vocab,
+                      memory.get(),
+                      n_embd,
+                      cparams.kv_unified ? LLAMA_MAX_SEQ : cparams.n_seq_max,
+                      output_all)) {
         LLAMA_LOG_ERROR("%s: failed to initialize batch\n", __func__);
         return -1;
     }
@@ -976,15 +1014,19 @@ int llama_context::decode(const llama_batch & batch_inp) {
     if (output_all) {
         // require that all tokens are output
         if (n_outputs_all != n_tokens_all) {
-            LLAMA_LOG_ERROR("%s: pooled embedding requires that all tokens are output (n_outputs_all = %d, n_tokens_all = %d)\n",
-                    __func__, n_outputs_all, n_tokens_all);
+            LLAMA_LOG_ERROR(
+                "%s: pooled embedding requires that all tokens are output (n_outputs_all = %d, n_tokens_all = %d)\n",
+                __func__,
+                n_outputs_all,
+                n_tokens_all);
             return -1;
         }
     }
 
     GGML_ASSERT(n_tokens_all <= cparams.n_batch);
 
-    GGML_ASSERT((cparams.causal_attn || cparams.n_ubatch >= n_tokens_all) && "non-causal attention requires n_ubatch >= n_tokens");
+    GGML_ASSERT((cparams.causal_attn || cparams.n_ubatch >= n_tokens_all) &&
+                "non-causal attention requires n_ubatch >= n_tokens");
 
     if (t_compute_start_us == 0) {
         t_compute_start_us = ggml_time_us();
@@ -995,6 +1037,11 @@ int llama_context::decode(const llama_batch & batch_inp) {
     embd_seq.clear();
     output_swaps.clear();
 
+    // For mean pooling, we need to accumulate embeddings across all ubatches
+    // since sequences can span multiple ubatches
+    std::map<llama_seq_id, std::vector<float>> embd_accumulator;
+    std::map<llama_seq_id, int32_t>            token_counts;
+
     bool did_optimize = false;
 
     // handle any pending defrags/shifts
@@ -1011,7 +1058,8 @@ int llama_context::decode(const llama_batch & batch_inp) {
         switch (mctx->get_status()) {
             case LLAMA_MEMORY_STATUS_SUCCESS:
                 {
-                } break;
+                }
+                break;
             case LLAMA_MEMORY_STATUS_NO_UPDATE:
                 {
                     LLAMA_LOG_ERROR("%s: unexpected memory context status: %d\n", __func__, mctx->get_status());
@@ -1024,19 +1072,23 @@ int llama_context::decode(const llama_batch & batch_inp) {
                         did_optimize = true;
 
                         if (kv_self_update(true)) {
-                            LLAMA_LOG_DEBUG("%s: retrying batch size %d after cache optimization\n", __func__, balloc->get_n_tokens());
+                            LLAMA_LOG_DEBUG("%s: retrying batch size %d after cache optimization\n",
+                                            __func__,
+                                            balloc->get_n_tokens());
 
                             continue;
                         }
                     }
 
-                    LLAMA_LOG_WARN("%s: failed to find a memory slot for batch of size %d\n", __func__, balloc->get_n_tokens());
+                    LLAMA_LOG_WARN(
+                        "%s: failed to find a memory slot for batch of size %d\n", __func__, balloc->get_n_tokens());
 
                     return 1;
                 }
             case LLAMA_MEMORY_STATUS_FAILED_COMPUTE:
                 {
-                    LLAMA_LOG_ERROR("%s: compute failed while preparing batch of size %d\n", __func__, balloc->get_n_tokens());
+                    LLAMA_LOG_ERROR(
+                        "%s: compute failed while preparing batch of size %d\n", __func__, balloc->get_n_tokens());
 
                     return -2;
                 }
@@ -1072,7 +1124,7 @@ int llama_context::decode(const llama_batch & batch_inp) {
             n_outputs = n_outputs_new;
         }
 
-        ggml_status status;
+        ggml_status  status;
         const auto * res = process_ubatch(ubatch, LLM_GRAPH_TYPE_DECODER, mctx.get(), status);
 
         if (!res) {
@@ -1093,16 +1145,21 @@ int llama_context::decode(const llama_batch & batch_inp) {
                     continue;
                 }
 
-                LLAMA_LOG_WARN("%s: removing KV cache entries for seq_id = %d, pos = [%d, +inf)\n", __func__, s, pos_min[s]);
+                LLAMA_LOG_WARN(
+                    "%s: removing KV cache entries for seq_id = %d, pos = [%d, +inf)\n", __func__, s, pos_min[s]);
 
                 memory->seq_rm(s, pos_min[s], -1);
             }
 
             switch (status) {
-                case GGML_STATUS_ABORTED:      return  2;
-                case GGML_STATUS_ALLOC_FAILED: return -2;
-                case GGML_STATUS_FAILED:       return -3;
-                case GGML_STATUS_SUCCESS:      GGML_ABORT("should not happen");
+                case GGML_STATUS_ABORTED:
+                    return 2;
+                case GGML_STATUS_ALLOC_FAILED:
+                    return -2;
+                case GGML_STATUS_FAILED:
+                    return -3;
+                case GGML_STATUS_SUCCESS:
+                    GGML_ABORT("should not happen");
             }
         }
 
@@ -1124,12 +1181,13 @@ int llama_context::decode(const llama_batch & batch_inp) {
             GGML_ASSERT(backend_res != nullptr);
             GGML_ASSERT(logits != nullptr);
 
-            float * logits_out = logits + n_outputs_prev*n_vocab;
+            float * logits_out = logits + n_outputs_prev * n_vocab;
 
             if (n_outputs) {
-                GGML_ASSERT( n_outputs_prev + n_outputs <= n_outputs_all);
-                GGML_ASSERT((n_outputs_prev + n_outputs)*n_vocab <= (int64_t) logits_size);
-                ggml_backend_tensor_get_async(backend_res, t_logits, logits_out, 0, n_outputs*n_vocab*sizeof(float));
+                GGML_ASSERT(n_outputs_prev + n_outputs <= n_outputs_all);
+                GGML_ASSERT((n_outputs_prev + n_outputs) * n_vocab <= (int64_t) logits_size);
+                ggml_backend_tensor_get_async(
+                    backend_res, t_logits, logits_out, 0, n_outputs * n_vocab * sizeof(float));
             }
         }
 
@@ -1143,15 +1201,57 @@ int llama_context::decode(const llama_batch & batch_inp) {
                     {
                         // extract token embeddings
                         GGML_ASSERT(embd != nullptr);
-                        float * embd_out = embd + n_outputs_prev*n_embd;
+                        float * embd_out = embd + n_outputs_prev * n_embd;
 
                         if (n_outputs) {
-                            GGML_ASSERT( n_outputs_prev + n_outputs <= n_outputs_all);
-                            GGML_ASSERT((n_outputs_prev + n_outputs)*n_embd <= (int64_t) embd_size);
-                            ggml_backend_tensor_get_async(backend_embd, t_embd, embd_out, 0, n_outputs*n_embd*sizeof(float));
+                            GGML_ASSERT(n_outputs_prev + n_outputs <= n_outputs_all);
+                            GGML_ASSERT((n_outputs_prev + n_outputs) * n_embd <= (int64_t) embd_size);
+                            ggml_backend_tensor_get_async(
+                                backend_embd, t_embd, embd_out, 0, n_outputs * n_embd * sizeof(float));
                         }
-                    } break;
+                    }
+                    break;
                 case LLAMA_POOLING_TYPE_MEAN:
+                    {
+                        // For mean pooling, we need to accumulate embeddings across all ubatches
+                        // since sequences can span multiple ubatches
+                        for (uint32_t s = 0; s < ubatch.n_seqs_unq; ++s) {
+                            const llama_seq_id seq_id  = ubatch.seq_id_unq[s];
+                            const int32_t      seq_idx = ubatch.seq_idx[seq_id];
+
+                            // Get the embedding for this sequence in this ubatch
+                            std::vector<float> ubatch_embd(n_embd);
+                            ggml_backend_tensor_get_async(backend_embd,
+                                                          t_embd,
+                                                          ubatch_embd.data(),
+                                                          (n_embd * seq_idx) * sizeof(float),
+                                                          n_embd * sizeof(float));
+
+                            // Synchronize to get the data
+                            ggml_backend_synchronize(backend_embd);
+
+                            // Count tokens for this sequence in this ubatch
+                            int32_t ubatch_tokens = 0;
+                            for (uint32_t i = 0; i < ubatch.n_tokens; ++i) {
+                                if (ubatch.seq_id[i][0] == seq_id) {
+                                    ubatch_tokens++;
+                                }
+                            }
+
+                            // Initialize accumulator if this is the first time we see this sequence
+                            if (embd_accumulator.find(seq_id) == embd_accumulator.end()) {
+                                embd_accumulator[seq_id] = std::vector<float>(n_embd, 0.0f);
+                                token_counts[seq_id]     = 0;
+                            }
+
+                            // Accumulate the weighted embedding (multiply by number of tokens)
+                            for (int64_t j = 0; j < n_embd; ++j) {
+                                embd_accumulator[seq_id][j] += ubatch_embd[j] * ubatch_tokens;
+                            }
+                            token_counts[seq_id] += ubatch_tokens;
+                        }
+                    }
+                    break;
                 case LLAMA_POOLING_TYPE_CLS:
                 case LLAMA_POOLING_TYPE_LAST:
                     {
@@ -1163,9 +1263,14 @@ int llama_context::decode(const llama_batch & batch_inp) {
                             const int32_t      seq_idx = ubatch.seq_idx[seq_id];
 
                             embd_seq_out[seq_id].resize(n_embd);
-                            ggml_backend_tensor_get_async(backend_embd, t_embd, embd_seq_out[seq_id].data(), (n_embd*seq_idx)*sizeof(float), n_embd*sizeof(float));
+                            ggml_backend_tensor_get_async(backend_embd,
+                                                          t_embd,
+                                                          embd_seq_out[seq_id].data(),
+                                                          (n_embd * seq_idx) * sizeof(float),
+                                                          n_embd * sizeof(float));
                         }
-                    } break;
+                    }
+                    break;
                 case LLAMA_POOLING_TYPE_RANK:
                     {
                         // extract the rerank score - n_cls_out floats per sequence
@@ -1178,9 +1283,14 @@ int llama_context::decode(const llama_batch & batch_inp) {
                             const int32_t      seq_idx = ubatch.seq_idx[seq_id];
 
                             embd_seq_out[seq_id].resize(n_cls_out);
-                            ggml_backend_tensor_get_async(backend_embd, t_embd, embd_seq_out[seq_id].data(), (n_cls_out*seq_idx)*sizeof(float), n_cls_out*sizeof(float));
+                            ggml_backend_tensor_get_async(backend_embd,
+                                                          t_embd,
+                                                          embd_seq_out[seq_id].data(),
+                                                          (n_cls_out * seq_idx) * sizeof(float),
+                                                          n_cls_out * sizeof(float));
                         }
-                    } break;
+                    }
+                    break;
                 case LLAMA_POOLING_TYPE_UNSPECIFIED:
                     {
                         GGML_ABORT("unknown pooling type");
@@ -1191,6 +1301,26 @@ int llama_context::decode(const llama_batch & batch_inp) {
         n_outputs_prev += n_outputs;
     } while (mctx->next());
 
+    // For mean pooling, compute the final mean across all accumulated embeddings
+    if (cparams.pooling_type == LLAMA_POOLING_TYPE_MEAN && !embd_accumulator.empty()) {
+        auto & embd_seq_out = embd_seq;
+
+        for (const auto & [seq_id, accumulated_embd] : embd_accumulator) {
+            const int32_t total_tokens = token_counts[seq_id];
+
+            if (total_tokens > 0) {
+                embd_seq_out[seq_id].resize(n_embd);
+
+                // Compute the mean by dividing by total tokens
+                for (int64_t j = 0; j < n_embd; ++j) {
+                    embd_seq_out[seq_id][j] = accumulated_embd[j] / total_tokens;
+                }
+
+                // Debug output removed for production
+            }
+        }
+    }
+
     // set to total number of outputs in the batch, for use in llama_get_logits_ith
     n_outputs = n_outputs_all;
 
@@ -1203,7 +1333,7 @@ int llama_context::decode(const llama_batch & batch_inp) {
         GGML_ASSERT(out_ids.size() == (size_t) n_outputs);
 
         for (int64_t i = 0; i < n_outputs; ++i) {
-            int64_t out_id = out_ids[i];
+            int64_t out_id     = out_ids[i];
             output_ids[out_id] = i;
             if (out_id != i) {
                 sorted_output = false;
@@ -1276,8 +1406,8 @@ uint32_t llama_context::output_reserve(int32_t n_outputs) {
         has_embd   = true;
     }
 
-    logits_size = has_logits ? n_vocab*n_outputs_max : 0;
-    embd_size   = has_embd   ?  n_embd*n_outputs_max : 0;
+    logits_size = has_logits ? n_vocab * n_outputs_max : 0;
+    embd_size   = has_embd ? n_embd * n_outputs_max : 0;
 
     if (output_ids.empty()) {
         // init, never resized afterwards
@@ -1293,31 +1423,35 @@ uint32_t llama_context::output_reserve(int32_t n_outputs) {
         if (buf_output) {
 #ifndef NDEBUG
             // This doesn't happen often, but may be annoying in some cases (like the HellaSwag benchmark)
-            LLAMA_LOG_INFO("%s: reallocating output buffer from size %.02f MiB to %.02f MiB\n", __func__, prev_size / 1024.0 / 1024.0, new_size / 1024.0 / 1024.0);
+            LLAMA_LOG_INFO("%s: reallocating output buffer from size %.02f MiB to %.02f MiB\n",
+                           __func__,
+                           prev_size / 1024.0 / 1024.0,
+                           new_size / 1024.0 / 1024.0);
 #endif
             buf_output = nullptr;
-            logits = nullptr;
-            embd = nullptr;
+            logits     = nullptr;
+            embd       = nullptr;
         }
 
-        auto * buft = ggml_backend_cpu_buffer_type();
+        auto * buft                 = ggml_backend_cpu_buffer_type();
         // try to use the host buffer of the device where the output tensor is allocated for faster transfer to system memory
-        auto * output_dev = model.dev_output();
+        auto * output_dev           = model.dev_output();
         auto * output_dev_host_buft = output_dev ? ggml_backend_dev_host_buffer_type(output_dev) : nullptr;
         if (output_dev_host_buft) {
             buft = output_dev_host_buft;
         }
         buf_output.reset(ggml_backend_buft_alloc_buffer(buft, new_size));
         if (buf_output == nullptr) {
-            LLAMA_LOG_ERROR("%s: failed to allocate output buffer of size %.2f MiB\n", __func__, new_size / (1024.0 * 1024.0));
+            LLAMA_LOG_ERROR(
+                "%s: failed to allocate output buffer of size %.2f MiB\n", __func__, new_size / (1024.0 * 1024.0));
             return 0;
         }
     }
 
     float * output_base = (float *) ggml_backend_buffer_get_base(buf_output.get());
 
-    logits = has_logits ? output_base               : nullptr;
-    embd   = has_embd   ? output_base + logits_size : nullptr;
+    logits = has_logits ? output_base : nullptr;
+    embd   = has_embd ? output_base + logits_size : nullptr;
 
     // set all ids as invalid (negative)
     std::fill(output_ids.begin(), output_ids.end(), -1);
@@ -1337,13 +1471,13 @@ void llama_context::output_reorder() {
 
         if (logits_size > 0) {
             for (uint64_t k = 0; k < n_vocab; k++) {
-                std::swap(logits[i0*n_vocab + k], logits[i1*n_vocab + k]);
+                std::swap(logits[i0 * n_vocab + k], logits[i1 * n_vocab + k]);
             }
         }
 
         if (embd_size > 0) {
             for (uint64_t k = 0; k < n_embd; k++) {
-                std::swap(embd[i0*n_embd + k], embd[i1*n_embd + k]);
+                std::swap(embd[i0 * n_embd + k], embd[i1 * n_embd + k]);
             }
         }
     }
@@ -1356,21 +1490,32 @@ void llama_context::output_reorder() {
 //
 
 uint32_t llama_context::graph_max_nodes() const {
-    return std::max<uint32_t>(1024u, 8u*model.n_tensors());
+    return std::max<uint32_t>(1024u, 8u * model.n_tensors());
 }
 
 llm_graph_result * llama_context::get_gf_res_reserve() const {
     return static_cast<llm_graph_result *>(gf_res_reserve.get());
 }
 
-ggml_cgraph * llama_context::graph_reserve(uint32_t n_tokens, uint32_t n_seqs, uint32_t n_outputs, const llama_memory_context_i * mctx) {
-    LLAMA_LOG_DEBUG("%s: reserving a graph for ubatch with n_tokens = %4u, n_seqs = %2u, n_outputs = %4u\n", __func__, n_tokens, n_seqs, n_outputs);
+ggml_cgraph * llama_context::graph_reserve(uint32_t                       n_tokens,
+                                           uint32_t                       n_seqs,
+                                           uint32_t                       n_outputs,
+                                           const llama_memory_context_i * mctx) {
+    LLAMA_LOG_DEBUG("%s: reserving a graph for ubatch with n_tokens = %4u, n_seqs = %2u, n_outputs = %4u\n",
+                    __func__,
+                    n_tokens,
+                    n_seqs,
+                    n_outputs);
 
     if (n_tokens % n_seqs != 0) {
-        n_tokens = ((n_tokens + (n_seqs - 1)) / n_seqs) * n_seqs; // round to next multiple of n_seqs
+        n_tokens  = ((n_tokens + (n_seqs - 1)) / n_seqs) * n_seqs;  // round to next multiple of n_seqs
         n_outputs = std::min(n_outputs, n_tokens);
 
-        LLAMA_LOG_DEBUG("%s: making n_tokens a multiple of n_seqs - n_tokens = %u, n_seqs = %u, n_outputs = %u\n", __func__, n_tokens, n_seqs, n_outputs);
+        LLAMA_LOG_DEBUG("%s: making n_tokens a multiple of n_seqs - n_tokens = %u, n_seqs = %u, n_outputs = %u\n",
+                        __func__,
+                        n_tokens,
+                        n_seqs,
+                        n_outputs);
     }
 
     ggml_backend_sched_reset(sched.get());
@@ -1385,7 +1530,7 @@ ggml_cgraph * llama_context::graph_reserve(uint32_t n_tokens, uint32_t n_seqs, u
     this->n_outputs = n_outputs;
 
     llama_batch_allocr balloc(model.hparams.n_pos_per_embd());
-    llama_ubatch ubatch = balloc.ubatch_reserve(n_tokens/n_seqs, n_seqs);
+    llama_ubatch       ubatch = balloc.ubatch_reserve(n_tokens / n_seqs, n_seqs);
 
     auto * res = gf_res_reserve.get();
 
@@ -1406,38 +1551,36 @@ ggml_cgraph * llama_context::graph_reserve(uint32_t n_tokens, uint32_t n_seqs, u
     return gf;
 }
 
-llm_graph_params llama_context::graph_params(
-                        llm_graph_result * res,
-                      const llama_ubatch & ubatch,
-            const llama_memory_context_i * mctx,
-            llm_graph_type   gtype) const {
+llm_graph_params llama_context::graph_params(llm_graph_result *             res,
+                                             const llama_ubatch &           ubatch,
+                                             const llama_memory_context_i * mctx,
+                                             llm_graph_type                 gtype) const {
     return {
-        /*.arch        =*/ model.arch,
-        /*.hparams     =*/ model.hparams,
-        /*.cparams     =*/ cparams,
-        /*.ubatch      =*/ ubatch,
-        /*.gtype       =*/ gtype,
-        /*.sched       =*/ sched.get(),
-        /*.backend_cpu =*/ backend_cpu,
-        /*.cvec        =*/ &cvec,
-        /*.loras       =*/ &loras,
-        /*.mctx        =*/ mctx,
-        /*.cross       =*/ &cross,
-        /*.n_outputs   =*/ n_outputs,
-        /*.cb          =*/ graph_get_cb(),
-        /*.res         =*/ res,
+        /*.arch        =*/model.arch,
+        /*.hparams     =*/model.hparams,
+        /*.cparams     =*/cparams,
+        /*.ubatch      =*/ubatch,
+        /*.gtype       =*/gtype,
+        /*.sched       =*/sched.get(),
+        /*.backend_cpu =*/backend_cpu,
+        /*.cvec        =*/&cvec,
+        /*.loras       =*/&loras,
+        /*.mctx        =*/mctx,
+        /*.cross       =*/&cross,
+        /*.n_outputs   =*/n_outputs,
+        /*.cb          =*/graph_get_cb(),
+        /*.res         =*/res,
     };
 }
 
-ggml_status llama_context::graph_compute(
-            ggml_cgraph * gf,
-                   bool   batched) {
-    int n_threads        = batched ? cparams.n_threads_batch : cparams.n_threads;
-    ggml_threadpool_t tp = batched ? threadpool_batch        : threadpool;
+ggml_status llama_context::graph_compute(ggml_cgraph * gf, bool batched) {
+    int               n_threads = batched ? cparams.n_threads_batch : cparams.n_threads;
+    ggml_threadpool_t tp        = batched ? threadpool_batch : threadpool;
 
     if (backend_cpu != nullptr) {
-        auto * reg = ggml_backend_dev_backend_reg(ggml_backend_get_device(backend_cpu));
-        auto * set_threadpool_fn = (decltype(ggml_backend_cpu_set_threadpool) *) ggml_backend_reg_get_proc_address(reg, "ggml_backend_cpu_set_threadpool");
+        auto * reg               = ggml_backend_dev_backend_reg(ggml_backend_get_device(backend_cpu));
+        auto * set_threadpool_fn = (decltype(ggml_backend_cpu_set_threadpool) *) ggml_backend_reg_get_proc_address(
+            reg, "ggml_backend_cpu_set_threadpool");
         set_threadpool_fn(backend_cpu, tp);
     }
 
@@ -1494,29 +1637,24 @@ llm_graph_cb llama_context::graph_get_cb() const {
 //
 
 class llama_io_write_dummy : public llama_io_write_i {
-public:
+  public:
     llama_io_write_dummy() = default;
 
-    void write(const void * /* src */, size_t size) override {
-        size_written += size;
-    }
+    void write(const void * /* src */, size_t size) override { size_written += size; }
 
     void write_tensor(const ggml_tensor * /* tensor */, size_t /* offset */, size_t size) override {
         size_written += size;
     }
 
-    size_t n_bytes() override {
-        return size_written;
-    }
+    size_t n_bytes() override { return size_written; }
 
-private:
+  private:
     size_t size_written = 0;
 };
 
 class llama_io_write_buffer : public llama_io_write_i {
-public:
-    llama_io_write_buffer(
-            uint8_t * p, size_t len) : ptr(p), buf_size(len) {}
+  public:
+    llama_io_write_buffer(uint8_t * p, size_t len) : ptr(p), buf_size(len) {}
 
     void write(const void * src, size_t size) override {
         if (size > buf_size) {
@@ -1538,18 +1676,16 @@ public:
         buf_size -= size;
     }
 
-    size_t n_bytes() override {
-        return size_written;
-    }
+    size_t n_bytes() override { return size_written; }
 
-private:
+  private:
     uint8_t * ptr;
-    size_t buf_size = 0;
-    size_t size_written = 0;
+    size_t    buf_size     = 0;
+    size_t    size_written = 0;
 };
 
 class llama_io_read_buffer : public llama_io_read_i {
-public:
+  public:
     llama_io_read_buffer(const uint8_t * p, size_t len) : ptr(p), buf_size(len) {}
 
     const uint8_t * read(size_t size) override {
@@ -1563,22 +1699,18 @@ public:
         return base_ptr;
     }
 
-    void read_to(void * dst, size_t size) override {
-        memcpy(dst, read(size), size);
-    }
+    void read_to(void * dst, size_t size) override { memcpy(dst, read(size), size); }
 
-    size_t n_bytes() override {
-        return size_read;
-    }
+    size_t n_bytes() override { return size_read; }
 
-private:
+  private:
     const uint8_t * ptr;
-    size_t buf_size = 0;
-    size_t size_read = 0;
+    size_t          buf_size  = 0;
+    size_t          size_read = 0;
 };
 
 class llama_io_write_file : public llama_io_write_i {
-public:
+  public:
     llama_io_write_file(llama_file * f) : file(f) {}
 
     void write(const void * src, size_t size) override {
@@ -1592,18 +1724,16 @@ public:
         write(temp_buffer.data(), temp_buffer.size());
     }
 
-    size_t n_bytes() override {
-        return size_written;
-    }
+    size_t n_bytes() override { return size_written; }
 
-private:
-    llama_file * file;
-    size_t size_written = 0;
+  private:
+    llama_file *         file;
+    size_t               size_written = 0;
     std::vector<uint8_t> temp_buffer;
 };
 
 class llama_io_read_file : public llama_io_read_i {
-public:
+  public:
     llama_io_read_file(llama_file * f) : file(f) {}
 
     void read_to(void * dst, size_t size) override {
@@ -1617,13 +1747,11 @@ public:
         return temp_buffer.data();
     }
 
-    size_t n_bytes() override {
-        return size_read;
-    }
+    size_t n_bytes() override { return size_read; }
 
-private:
-    llama_file * file;
-    size_t size_read = 0;
+  private:
+    llama_file *         file;
+    size_t               size_read = 0;
     std::vector<uint8_t> temp_buffer;
 };
 
@@ -1687,7 +1815,10 @@ size_t llama_context::state_seq_set_data(llama_seq_id seq_id, const uint8_t * sr
     }
 }
 
-bool llama_context::state_load_file(const char * filepath, llama_token * tokens_out, size_t n_token_capacity, size_t * n_token_count_out) {
+bool llama_context::state_load_file(const char *  filepath,
+                                    llama_token * tokens_out,
+                                    size_t        n_token_capacity,
+                                    size_t *      n_token_count_out) {
     llama_file file(filepath, "rb");
 
     // sanity checks
@@ -1706,7 +1837,10 @@ bool llama_context::state_load_file(const char * filepath, llama_token * tokens_
         const uint32_t n_token_count = file.read_u32();
 
         if (n_token_count > n_token_capacity) {
-            LLAMA_LOG_ERROR("%s: token count in session file exceeded capacity! %u > %zu\n", __func__, n_token_count, n_token_capacity);
+            LLAMA_LOG_ERROR("%s: token count in session file exceeded capacity! %u > %zu\n",
+                            __func__,
+                            n_token_count,
+                            n_token_capacity);
             return false;
         }
 
@@ -1718,11 +1852,14 @@ bool llama_context::state_load_file(const char * filepath, llama_token * tokens_
     {
         const size_t n_state_size_cur = file.size() - file.tell();
 
-        llama_io_read_file io( &file);
-        const size_t n_read = state_read_data(io);
+        llama_io_read_file io(&file);
+        const size_t       n_read = state_read_data(io);
 
         if (n_read != n_state_size_cur) {
-            LLAMA_LOG_ERROR("%s: did not read all of the session file data! size %zu, got %zu\n", __func__, n_state_size_cur, n_read);
+            LLAMA_LOG_ERROR("%s: did not read all of the session file data! size %zu, got %zu\n",
+                            __func__,
+                            n_state_size_cur,
+                            n_read);
             return false;
         }
     }
@@ -1747,7 +1884,11 @@ bool llama_context::state_save_file(const char * filepath, const llama_token * t
     return true;
 }
 
-size_t llama_context::state_seq_load_file(llama_seq_id seq_id, const char * filepath, llama_token * tokens_out, size_t n_token_capacity, size_t * n_token_count_out) {
+size_t llama_context::state_seq_load_file(llama_seq_id  seq_id,
+                                          const char *  filepath,
+                                          llama_token * tokens_out,
+                                          size_t        n_token_capacity,
+                                          size_t *      n_token_count_out) {
     llama_file file(filepath, "rb");
 
     // version checks
@@ -1756,7 +1897,8 @@ size_t llama_context::state_seq_load_file(llama_seq_id seq_id, const char * file
         const uint32_t version = file.read_u32();
 
         if (magic != LLAMA_STATE_SEQ_MAGIC || version != LLAMA_STATE_SEQ_VERSION) {
-            LLAMA_LOG_ERROR("%s: unknown (magic, version) for sequence state file: %08x, %08x\n", __func__, magic, version);
+            LLAMA_LOG_ERROR(
+                "%s: unknown (magic, version) for sequence state file: %08x, %08x\n", __func__, magic, version);
             return 0;
         }
     }
@@ -1766,7 +1908,10 @@ size_t llama_context::state_seq_load_file(llama_seq_id seq_id, const char * file
         const uint32_t n_token_count = file.read_u32();
 
         if (n_token_count > n_token_capacity) {
-            LLAMA_LOG_ERROR("%s: token count in sequence state file exceeded capacity! %u > %zu\n", __func__, n_token_count, n_token_capacity);
+            LLAMA_LOG_ERROR("%s: token count in sequence state file exceeded capacity! %u > %zu\n",
+                            __func__,
+                            n_token_count,
+                            n_token_capacity);
             return 0;
         }
 
@@ -1776,9 +1921,9 @@ size_t llama_context::state_seq_load_file(llama_seq_id seq_id, const char * file
 
     // restore the context state
     {
-        const size_t state_size = file.size() - file.tell();
+        const size_t       state_size = file.size() - file.tell();
         llama_io_read_file io(&file);
-        const size_t nread = state_seq_read_data(io, seq_id);
+        const size_t       nread = state_seq_read_data(io, seq_id);
         if (!nread) {
             LLAMA_LOG_ERROR("%s: failed to restore sequence state\n", __func__);
             return 0;
@@ -1790,7 +1935,10 @@ size_t llama_context::state_seq_load_file(llama_seq_id seq_id, const char * file
     return file.tell();
 }
 
-size_t llama_context::state_seq_save_file(llama_seq_id seq_id, const char * filepath, const llama_token * tokens, size_t n_token_count) {
+size_t llama_context::state_seq_save_file(llama_seq_id        seq_id,
+                                          const char *        filepath,
+                                          const llama_token * tokens,
+                                          size_t              n_token_count) {
     llama_file file(filepath, "wb");
 
     file.write_u32(LLAMA_STATE_SEQ_MAGIC);
@@ -1826,7 +1974,7 @@ size_t llama_context::state_write_data(llama_io_write_i & io) {
     {
         LLAMA_LOG_DEBUG("%s: - writing output ids\n", __func__);
 
-        const auto n_outputs    = this->n_outputs;
+        const auto   n_outputs  = this->n_outputs;
         const auto & output_ids = this->output_ids;
 
         std::vector<int32_t> w_output_pos;
@@ -1854,7 +2002,8 @@ size_t llama_context::state_write_data(llama_io_write_i & io) {
     {
         LLAMA_LOG_DEBUG("%s: - writing logits\n", __func__);
 
-        const uint64_t logits_size = std::min((uint64_t) this->logits_size, (uint64_t) n_outputs * model.vocab.n_tokens());
+        const uint64_t logits_size =
+            std::min((uint64_t) this->logits_size, (uint64_t) n_outputs * model.vocab.n_tokens());
 
         io.write(&logits_size, sizeof(logits_size));
 
@@ -1896,7 +2045,8 @@ size_t llama_context::state_read_data(llama_io_read_i & io) {
         std::string arch_str;
         io.read_string(arch_str);
         if (cur_arch_str != arch_str) {
-            throw std::runtime_error(format("wrong model arch: '%s' instead of '%s'", arch_str.c_str(), cur_arch_str.c_str()));
+            throw std::runtime_error(
+                format("wrong model arch: '%s' instead of '%s'", arch_str.c_str(), cur_arch_str.c_str()));
         }
         // TODO: add more info which needs to be identical but which is not verified otherwise
     }
@@ -1921,7 +2071,8 @@ size_t llama_context::state_read_data(llama_io_read_i & io) {
             for (int32_t i = 0; i < (int32_t) output_pos.size(); ++i) {
                 int32_t id = output_pos[i];
                 if ((uint32_t) id >= n_batch()) {
-                    throw std::runtime_error(format("invalid output id, %d does not fit in batch size of %u", id, n_batch()));
+                    throw std::runtime_error(
+                        format("invalid output id, %d does not fit in batch size of %u", id, n_batch()));
                 }
                 this->output_ids[id] = i;
             }
@@ -2010,10 +2161,10 @@ llama_perf_context_data llama_context::perf_get_data() const {
 }
 
 void llama_context::perf_reset() {
-    t_start_us  = ggml_time_us();
-    t_eval_us   = n_eval = 0;
+    t_start_us = ggml_time_us();
+    t_eval_us = n_eval = 0;
     t_p_eval_us = n_p_eval = 0;
-    n_reused    = 0;
+    n_reused               = 0;
 }
 
 //
@@ -2028,10 +2179,10 @@ static void llama_set_param(struct ggml_tensor * tensor, llama_opt_param_filter
         return;
     }
     if (strcmp(tensor->name, "token_embd.weight") == 0) {
-        return; // FIXME
+        return;  // FIXME
     }
     if (strcmp(tensor->name, "rope_freqs.weight") == 0) {
-        return; // FIXME
+        return;  // FIXME
     }
     ggml_set_param(tensor);
 }
@@ -2039,10 +2190,10 @@ static void llama_set_param(struct ggml_tensor * tensor, llama_opt_param_filter
 void llama_context::opt_init(struct llama_model * model, struct llama_opt_params lopt_params) {
     GGML_ASSERT(!opt_ctx);
     model->hparams.n_ctx_train = lopt_params.n_ctx_train > 0 ? lopt_params.n_ctx_train : n_ctx();
-    const uint32_t n_batch     = std::min(this->n_batch(),  model->hparams.n_ctx_train);
+    const uint32_t n_batch     = std::min(this->n_batch(), model->hparams.n_ctx_train);
     const uint32_t n_ubatch    = std::min(this->n_ubatch(), n_batch);
-    GGML_ASSERT(model->hparams.n_ctx_train % n_batch  == 0);
-    GGML_ASSERT(n_batch                    % n_ubatch == 0);
+    GGML_ASSERT(model->hparams.n_ctx_train % n_batch == 0);
+    GGML_ASSERT(n_batch % n_ubatch == 0);
 
     ggml_opt_params opt_params = ggml_opt_default_params(sched.get(), GGML_OPT_LOSS_TYPE_CROSS_ENTROPY);
     opt_params.opt_period      = n_batch / n_ubatch;
@@ -2051,45 +2202,44 @@ void llama_context::opt_init(struct llama_model * model, struct llama_opt_params
 
     opt_ctx = ggml_opt_init(opt_params);
 
-    llama_opt_param_filter param_filter = lopt_params.param_filter;
-    void * param_filter_ud              = lopt_params.param_filter_ud;
-
-  //llama_set_param(model->tok_embd,        param_filter, param_filter_ud); // FIXME
-    llama_set_param(model->type_embd,       param_filter, param_filter_ud);
-    llama_set_param(model->pos_embd,        param_filter, param_filter_ud);
-    llama_set_param(model->tok_norm,        param_filter, param_filter_ud);
-    llama_set_param(model->tok_norm_b,      param_filter, param_filter_ud);
-    llama_set_param(model->output_norm,     param_filter, param_filter_ud);
-    llama_set_param(model->output_norm_b,   param_filter, param_filter_ud);
-    llama_set_param(model->output,          param_filter, param_filter_ud);
-    llama_set_param(model->output_b,        param_filter, param_filter_ud);
+    llama_opt_param_filter param_filter    = lopt_params.param_filter;
+    void *                 param_filter_ud = lopt_params.param_filter_ud;
+
+    //llama_set_param(model->tok_embd,        param_filter, param_filter_ud); // FIXME
+    llama_set_param(model->type_embd, param_filter, param_filter_ud);
+    llama_set_param(model->pos_embd, param_filter, param_filter_ud);
+    llama_set_param(model->tok_norm, param_filter, param_filter_ud);
+    llama_set_param(model->tok_norm_b, param_filter, param_filter_ud);
+    llama_set_param(model->output_norm, param_filter, param_filter_ud);
+    llama_set_param(model->output_norm_b, param_filter, param_filter_ud);
+    llama_set_param(model->output, param_filter, param_filter_ud);
+    llama_set_param(model->output_b, param_filter, param_filter_ud);
     llama_set_param(model->output_norm_enc, param_filter, param_filter_ud);
-    llama_set_param(model->cls,             param_filter, param_filter_ud);
-    llama_set_param(model->cls_b,           param_filter, param_filter_ud);
-    llama_set_param(model->cls_out,         param_filter, param_filter_ud);
-    llama_set_param(model->cls_out_b,       param_filter, param_filter_ud);
+    llama_set_param(model->cls, param_filter, param_filter_ud);
+    llama_set_param(model->cls_b, param_filter, param_filter_ud);
+    llama_set_param(model->cls_out, param_filter, param_filter_ud);
+    llama_set_param(model->cls_out_b, param_filter, param_filter_ud);
 
     for (struct llama_layer & layer : model->layers) {
-        for (size_t i = 0; i < sizeof(layer)/sizeof(struct ggml_tensor *); ++i) {
+        for (size_t i = 0; i < sizeof(layer) / sizeof(struct ggml_tensor *); ++i) {
             llama_set_param(reinterpret_cast<struct ggml_tensor **>(&layer)[i], param_filter, param_filter_ud);
         }
     }
 }
 
-void llama_context::opt_epoch_iter(
-        ggml_opt_dataset_t               dataset,
-        ggml_opt_result_t                result,
-        const std::vector<llama_token> & tokens,
-        const std::vector<llama_token> & labels_sparse,
-        llama_batch                    & batch,
-        ggml_opt_epoch_callback          callback,
-        bool                             train,
-        int64_t                          idata_in_loop,
-        int64_t                          ndata_in_loop,
-        int64_t                          t_loop_start) {
+void llama_context::opt_epoch_iter(ggml_opt_dataset_t               dataset,
+                                   ggml_opt_result_t                result,
+                                   const std::vector<llama_token> & tokens,
+                                   const std::vector<llama_token> & labels_sparse,
+                                   llama_batch &                    batch,
+                                   ggml_opt_epoch_callback          callback,
+                                   bool                             train,
+                                   int64_t                          idata_in_loop,
+                                   int64_t                          ndata_in_loop,
+                                   int64_t                          t_loop_start) {
     GGML_ASSERT(opt_ctx);
     const uint32_t n_ctx    = llama_model_n_ctx_train(&model);
-    const uint32_t n_batch  = std::min(this->n_batch(),  n_ctx);
+    const uint32_t n_batch  = std::min(this->n_batch(), n_ctx);
     const uint32_t n_ubatch = std::min(this->n_ubatch(), n_batch);
 
     memory->clear(true);
@@ -2097,14 +2247,19 @@ void llama_context::opt_epoch_iter(
     for (uint32_t pos_ctx = 0; pos_ctx < n_ctx; pos_ctx += n_batch) {
         batch.n_tokens = n_batch;
         for (uint32_t pos_batch = 0; pos_batch < n_batch; ++pos_batch) {
-            batch.token   [pos_batch]    = tokens[pos_ctx + pos_batch];
-            batch.pos     [pos_batch]    = pos_ctx + pos_batch;
-            batch.n_seq_id[pos_batch]    = 1;
-            batch.seq_id  [pos_batch][0] = 0;
-            batch.logits  [pos_batch]    = true;
-        }
-
-        if (!balloc->init(batch, model.vocab, nullptr, model.hparams.n_embd, cparams.kv_unified ? LLAMA_MAX_SEQ : cparams.n_seq_max, true)) {
+            batch.token[pos_batch]     = tokens[pos_ctx + pos_batch];
+            batch.pos[pos_batch]       = pos_ctx + pos_batch;
+            batch.n_seq_id[pos_batch]  = 1;
+            batch.seq_id[pos_batch][0] = 0;
+            batch.logits[pos_batch]    = true;
+        }
+
+        if (!balloc->init(batch,
+                          model.vocab,
+                          nullptr,
+                          model.hparams.n_embd,
+                          cparams.kv_unified ? LLAMA_MAX_SEQ : cparams.n_seq_max,
+                          true)) {
             LLAMA_LOG_ERROR("%s: failed to initialize batch\n", __func__);
             return;
         }
@@ -2151,14 +2306,16 @@ void llama_context::opt_epoch_iter(
             struct ggml_context * ctx_compute_opt;
             {
                 const size_t size_gf = ggml_graph_size(gf);
-                const size_t size_meta = 4*size_gf*ggml_tensor_overhead() + 2*ggml_graph_overhead_custom(size_gf, /*grads = */ true);
+                const size_t size_meta =
+                    4 * size_gf * ggml_tensor_overhead() + 2 * ggml_graph_overhead_custom(size_gf, /*grads = */ true);
                 struct ggml_init_params params = {
-                    /*.mem_size   =*/ size_meta,
-                    /*.mem_buffer =*/ nullptr,
-                    /*.no_alloc   =*/ true,
+                    /*.mem_size   =*/size_meta,
+                    /*.mem_buffer =*/nullptr,
+                    /*.no_alloc   =*/true,
                 };
                 ctx_compute_opt = ggml_init(params);
             }
+
             ggml_opt_prepare_alloc(opt_ctx, ctx_compute_opt, gf, res->get_tokens(), res->get_logits());
             ggml_opt_alloc(opt_ctx, train);
 
@@ -2171,12 +2328,21 @@ void llama_context::opt_epoch_iter(
                 for (uint32_t pos_ubatch = 0; pos_ubatch < n_ubatch; ++pos_ubatch) {
                     const uint32_t ilabel = pos_ctx + pos_batch + pos_ubatch;
                     GGML_ASSERT(labels_sparse[ilabel] < labels->ne[0]);
-                    ggml_backend_tensor_set(labels, &onef, (pos_ubatch*labels->ne[0] + labels_sparse[ilabel])*sizeof(float), sizeof(float));
+                    ggml_backend_tensor_set(labels,
+                                            &onef,
+                                            (pos_ubatch * labels->ne[0] + labels_sparse[ilabel]) * sizeof(float),
+                                            sizeof(float));
                 }
             }
             ggml_opt_eval(opt_ctx, result);
             if (callback) {
-                callback(train, opt_ctx, dataset, result, idata_in_loop + (pos_ctx + pos_batch)/n_ubatch + 1, ndata_in_loop, t_loop_start);
+                callback(train,
+                         opt_ctx,
+                         dataset,
+                         result,
+                         idata_in_loop + (pos_ctx + pos_batch) / n_ubatch + 1,
+                         ndata_in_loop,
+                         t_loop_start);
             }
             ggml_free(ctx_compute_opt);
 
@@ -2185,49 +2351,66 @@ void llama_context::opt_epoch_iter(
     }
 }
 
-void llama_context::opt_epoch(
-        ggml_opt_dataset_t        dataset,
-        ggml_opt_result_t         result_train,
-        ggml_opt_result_t         result_eval,
-        int64_t                   idata_split,
-        ggml_opt_epoch_callback   callback_train,
-        ggml_opt_epoch_callback   callback_eval) {
+void llama_context::opt_epoch(ggml_opt_dataset_t      dataset,
+                              ggml_opt_result_t       result_train,
+                              ggml_opt_result_t       result_eval,
+                              int64_t                 idata_split,
+                              ggml_opt_epoch_callback callback_train,
+                              ggml_opt_epoch_callback callback_eval) {
     const uint32_t n_ctx    = this->n_ctx();
-    const uint32_t n_batch  = std::min(cparams.n_batch,  n_ctx);
+    const uint32_t n_batch  = std::min(cparams.n_batch, n_ctx);
     const uint32_t n_ubatch = std::min(cparams.n_ubatch, n_batch);
-    const  int64_t ndata    = ggml_opt_dataset_ndata(dataset);
+    const int64_t  ndata    = ggml_opt_dataset_ndata(dataset);
 
     GGML_ASSERT(idata_split >= 0);
     GGML_ASSERT(idata_split <= ndata);
 
     const uint32_t ubatch_per_ctx = n_ctx / n_ubatch;
 
-    struct llama_batch batch = llama_batch_init(n_batch, 0, 1);
-    std::vector<llama_token>        tokens(n_ctx);
+    struct llama_batch       batch = llama_batch_init(n_batch, 0, 1);
+    std::vector<llama_token> tokens(n_ctx);
     std::vector<llama_token> labels_sparse(n_ctx);
 
     int64_t idata = 0;
 
-    int64_t t_loop_start = ggml_time_us();
-    int64_t ndata_in_loop = idata_split*ubatch_per_ctx;
+    int64_t t_loop_start  = ggml_time_us();
+    int64_t ndata_in_loop = idata_split * ubatch_per_ctx;
     for (; idata < idata_split; ++idata) {
-        constexpr bool train = true;
-        const int64_t idata_in_loop = idata*ubatch_per_ctx;
-
-        ggml_opt_dataset_get_batch_host(dataset, tokens.data(), n_ctx*sizeof(llama_token), labels_sparse.data(), idata);
-        opt_epoch_iter(dataset, result_train, tokens, labels_sparse, batch,
-            callback_train, train, idata_in_loop, ndata_in_loop, t_loop_start);
-    }
-
-    t_loop_start = ggml_time_us();
-    ndata_in_loop = (ndata - idata_split)*ubatch_per_ctx;
+        constexpr bool train         = true;
+        const int64_t  idata_in_loop = idata * ubatch_per_ctx;
+
+        ggml_opt_dataset_get_batch_host(
+            dataset, tokens.data(), n_ctx * sizeof(llama_token), labels_sparse.data(), idata);
+        opt_epoch_iter(dataset,
+                       result_train,
+                       tokens,
+                       labels_sparse,
+                       batch,
+                       callback_train,
+                       train,
+                       idata_in_loop,
+                       ndata_in_loop,
+                       t_loop_start);
+    }
+
+    t_loop_start  = ggml_time_us();
+    ndata_in_loop = (ndata - idata_split) * ubatch_per_ctx;
     for (; idata < ndata; ++idata) {
-        constexpr bool train = false;
-        const int64_t idata_in_loop = (idata - idata_split)*ubatch_per_ctx;
-
-        ggml_opt_dataset_get_batch_host(dataset, tokens.data(), n_ctx*sizeof(llama_token), labels_sparse.data(), idata);
-        opt_epoch_iter(dataset, result_eval, tokens, labels_sparse, batch,
-            callback_eval, train, idata_in_loop, ndata_in_loop, t_loop_start);
+        constexpr bool train         = false;
+        const int64_t  idata_in_loop = (idata - idata_split) * ubatch_per_ctx;
+
+        ggml_opt_dataset_get_batch_host(
+            dataset, tokens.data(), n_ctx * sizeof(llama_token), labels_sparse.data(), idata);
+        opt_epoch_iter(dataset,
+                       result_eval,
+                       tokens,
+                       labels_sparse,
+                       batch,
+                       callback_eval,
+                       train,
+                       idata_in_loop,
+                       ndata_in_loop,
+                       t_loop_start);
     }
 
     llama_batch_free(batch);
@@ -2239,44 +2422,42 @@ void llama_context::opt_epoch(
 
 llama_context_params llama_context_default_params() {
     llama_context_params result = {
-        /*.n_ctx                       =*/ 512,
-        /*.n_batch                     =*/ 2048,
-        /*.n_ubatch                    =*/ 512,
-        /*.n_seq_max                   =*/ 1,
-        /*.n_threads                   =*/ GGML_DEFAULT_N_THREADS, // TODO: better default
-        /*.n_threads_batch             =*/ GGML_DEFAULT_N_THREADS,
-        /*.rope_scaling_type           =*/ LLAMA_ROPE_SCALING_TYPE_UNSPECIFIED,
-        /*.pooling_type                =*/ LLAMA_POOLING_TYPE_UNSPECIFIED,
-        /*.attention_type              =*/ LLAMA_ATTENTION_TYPE_UNSPECIFIED,
-        /*.rope_freq_base              =*/ 0.0f,
-        /*.rope_freq_scale             =*/ 0.0f,
-        /*.yarn_ext_factor             =*/ -1.0f,
-        /*.yarn_attn_factor            =*/ 1.0f,
-        /*.yarn_beta_fast              =*/ 32.0f,
-        /*.yarn_beta_slow              =*/ 1.0f,
-        /*.yarn_orig_ctx               =*/ 0,
-        /*.defrag_thold                =*/ -1.0f,
-        /*.cb_eval                     =*/ nullptr,
-        /*.cb_eval_user_data           =*/ nullptr,
-        /*.type_k                      =*/ GGML_TYPE_F16,
-        /*.type_v                      =*/ GGML_TYPE_F16,
-        /*.abort_callback              =*/ nullptr,
-        /*.abort_callback_data         =*/ nullptr,
-        /*.embeddings                  =*/ false,
-        /*.offload_kqv                 =*/ true,
-        /*.flash_attn                  =*/ false,
-        /*.no_perf                     =*/ true,
-        /*.op_offload                  =*/ true,
-        /*.swa_full                    =*/ true,
-        /*.kv_unified                  =*/ false,
+        /*.n_ctx                       =*/512,
+        /*.n_batch                     =*/2048,
+        /*.n_ubatch                    =*/512,
+        /*.n_seq_max                   =*/1,
+        /*.n_threads                   =*/GGML_DEFAULT_N_THREADS,  // TODO: better default
+        /*.n_threads_batch             =*/GGML_DEFAULT_N_THREADS,
+        /*.rope_scaling_type           =*/LLAMA_ROPE_SCALING_TYPE_UNSPECIFIED,
+        /*.pooling_type                =*/LLAMA_POOLING_TYPE_UNSPECIFIED,
+        /*.attention_type              =*/LLAMA_ATTENTION_TYPE_UNSPECIFIED,
+        /*.rope_freq_base              =*/0.0f,
+        /*.rope_freq_scale             =*/0.0f,
+        /*.yarn_ext_factor             =*/-1.0f,
+        /*.yarn_attn_factor            =*/1.0f,
+        /*.yarn_beta_fast              =*/32.0f,
+        /*.yarn_beta_slow              =*/1.0f,
+        /*.yarn_orig_ctx               =*/0,
+        /*.defrag_thold                =*/-1.0f,
+        /*.cb_eval                     =*/nullptr,
+        /*.cb_eval_user_data           =*/nullptr,
+        /*.type_k                      =*/GGML_TYPE_F16,
+        /*.type_v                      =*/GGML_TYPE_F16,
+        /*.abort_callback              =*/nullptr,
+        /*.abort_callback_data         =*/nullptr,
+        /*.embeddings                  =*/false,
+        /*.offload_kqv                 =*/true,
+        /*.flash_attn                  =*/false,
+        /*.no_perf                     =*/true,
+        /*.op_offload                  =*/true,
+        /*.swa_full                    =*/true,
+        /*.kv_unified                  =*/false,
     };
 
     return result;
 }
 
-llama_context * llama_init_from_model(
-                 llama_model * model,
-        llama_context_params   params) {
+llama_context * llama_init_from_model(llama_model * model, llama_context_params params) {
     if (!model) {
         LLAMA_LOG_ERROR("%s: model cannot be NULL\n", __func__);
         return nullptr;
@@ -2313,9 +2494,7 @@ llama_context * llama_init_from_model(
 }
 
 // deprecated
-llama_context * llama_new_context_with_model(
-                 llama_model * model,
-        llama_context_params   params) {
+llama_context * llama_new_context_with_model(llama_model * model, llama_context_params params) {
     return llama_init_from_model(model, params);
 }
 
@@ -2357,10 +2536,7 @@ enum llama_pooling_type llama_pooling_type(const llama_context * ctx) {
     return ctx->pooling_type();
 }
 
-void llama_attach_threadpool(
-            llama_context * ctx,
-        ggml_threadpool_t   threadpool,
-        ggml_threadpool_t   threadpool_batch) {
+void llama_attach_threadpool(llama_context * ctx, ggml_threadpool_t threadpool, ggml_threadpool_t threadpool_batch) {
     ctx->attach_threadpool(threadpool, threadpool_batch);
 }
 
@@ -2432,18 +2608,13 @@ float * llama_get_embeddings_seq(llama_context * ctx, llama_seq_id seq_id) {
 
 // llama adapter API
 
-int32_t llama_set_adapter_lora(
-            llama_context * ctx,
-            llama_adapter_lora * adapter,
-            float scale) {
+int32_t llama_set_adapter_lora(llama_context * ctx, llama_adapter_lora * adapter, float scale) {
     ctx->set_adapter_lora(adapter, scale);
 
     return 0;
 }
 
-int32_t llama_rm_adapter_lora(
-            llama_context * ctx,
-            llama_adapter_lora * adapter) {
+int32_t llama_rm_adapter_lora(llama_context * ctx, llama_adapter_lora * adapter) {
     bool res = ctx->rm_adapter_lora(adapter);
 
     return res ? 0 : -1;
@@ -2453,13 +2624,12 @@ void llama_clear_adapter_lora(llama_context * ctx) {
     ctx->clear_adapter_lora();
 }
 
-int32_t llama_apply_adapter_cvec(
-        llama_context * ctx,
-                 const float * data,
-                      size_t   len,
-                     int32_t   n_embd,
-                     int32_t   il_start,
-                     int32_t   il_end) {
+int32_t llama_apply_adapter_cvec(llama_context * ctx,
+                                 const float *   data,
+                                 size_t          len,
+                                 int32_t         n_embd,
+                                 int32_t         il_start,
+                                 int32_t         il_end) {
     bool res = ctx->apply_adapter_cvec(data, len, n_embd, il_start, il_end);
 
     return res ? 0 : -1;
@@ -2481,11 +2651,7 @@ void llama_memory_clear(llama_memory_t mem, bool data) {
     mem->clear(data);
 }
 
-bool llama_memory_seq_rm(
-        llama_memory_t mem,
-          llama_seq_id seq_id,
-             llama_pos p0,
-             llama_pos p1) {
+bool llama_memory_seq_rm(llama_memory_t mem, llama_seq_id seq_id, llama_pos p0, llama_pos p1) {
     if (!mem) {
         return true;
     }
@@ -2493,12 +2659,11 @@ bool llama_memory_seq_rm(
     return mem->seq_rm(seq_id, p0, p1);
 }
 
-void llama_memory_seq_cp(
-        llama_memory_t mem,
-          llama_seq_id seq_id_src,
-          llama_seq_id seq_id_dst,
-             llama_pos p0,
-             llama_pos p1) {
+void llama_memory_seq_cp(llama_memory_t mem,
+                         llama_seq_id   seq_id_src,
+                         llama_seq_id   seq_id_dst,
+                         llama_pos      p0,
+                         llama_pos      p1) {
     if (!mem) {
         return;
     }
@@ -2506,9 +2671,7 @@ void llama_memory_seq_cp(
     mem->seq_cp(seq_id_src, seq_id_dst, p0, p1);
 }
 
-void llama_memory_seq_keep(
-        llama_memory_t mem,
-          llama_seq_id seq_id) {
+void llama_memory_seq_keep(llama_memory_t mem, llama_seq_id seq_id) {
     if (!mem) {
         return;
     }
@@ -2516,12 +2679,7 @@ void llama_memory_seq_keep(
     mem->seq_keep(seq_id);
 }
 
-void llama_memory_seq_add(
-        llama_memory_t mem,
-          llama_seq_id seq_id,
-             llama_pos p0,
-             llama_pos p1,
-             llama_pos delta) {
+void llama_memory_seq_add(llama_memory_t mem, llama_seq_id seq_id, llama_pos p0, llama_pos p1, llama_pos delta) {
     if (!mem) {
         return;
     }
@@ -2529,12 +2687,7 @@ void llama_memory_seq_add(
     mem->seq_add(seq_id, p0, p1, delta);
 }
 
-void llama_memory_seq_div(
-        llama_memory_t mem,
-          llama_seq_id seq_id,
-             llama_pos p0,
-             llama_pos p1,
-                   int d) {
+void llama_memory_seq_div(llama_memory_t mem, llama_seq_id seq_id, llama_pos p0, llama_pos p1, int d) {
     if (!mem) {
         return;
     }
@@ -2542,9 +2695,7 @@ void llama_memory_seq_div(
     mem->seq_div(seq_id, p0, p1, d);
 }
 
-llama_pos llama_memory_seq_pos_min(
-        llama_memory_t mem,
-          llama_seq_id seq_id) {
+llama_pos llama_memory_seq_pos_min(llama_memory_t mem, llama_seq_id seq_id) {
     if (!mem) {
         return -1;
     }
@@ -2552,9 +2703,7 @@ llama_pos llama_memory_seq_pos_min(
     return mem->seq_pos_min(seq_id);
 }
 
-llama_pos llama_memory_seq_pos_max(
-        llama_memory_t mem,
-          llama_seq_id seq_id) {
+llama_pos llama_memory_seq_pos_max(llama_memory_t mem, llama_seq_id seq_id) {
     if (!mem) {
         return -1;
     }
@@ -2628,11 +2777,7 @@ void llama_kv_self_clear(llama_context * ctx) {
 }
 
 // deprecated
-bool llama_kv_self_seq_rm(
-        llama_context * ctx,
-         llama_seq_id   seq_id,
-            llama_pos   p0,
-            llama_pos   p1) {
+bool llama_kv_self_seq_rm(llama_context * ctx, llama_seq_id seq_id, llama_pos p0, llama_pos p1) {
     auto * kv = llama_get_memory(ctx);
     if (!kv) {
         return true;
@@ -2642,12 +2787,11 @@ bool llama_kv_self_seq_rm(
 }
 
 // deprecated
-void llama_kv_self_seq_cp(
-        llama_context * ctx,
-         llama_seq_id   seq_id_src,
-         llama_seq_id   seq_id_dst,
-            llama_pos   p0,
-            llama_pos   p1) {
+void llama_kv_self_seq_cp(llama_context * ctx,
+                          llama_seq_id    seq_id_src,
+                          llama_seq_id    seq_id_dst,
+                          llama_pos       p0,
+                          llama_pos       p1) {
     auto * kv = llama_get_memory(ctx);
     if (!kv) {
         return;
@@ -2667,12 +2811,7 @@ void llama_kv_self_seq_keep(llama_context * ctx, llama_seq_id seq_id) {
 }
 
 // deprecated
-void llama_kv_self_seq_add(
-        llama_context * ctx,
-         llama_seq_id   seq_id,
-            llama_pos   p0,
-            llama_pos   p1,
-            llama_pos   delta) {
+void llama_kv_self_seq_add(llama_context * ctx, llama_seq_id seq_id, llama_pos p0, llama_pos p1, llama_pos delta) {
     auto * kv = llama_get_memory(ctx);
     if (!kv) {
         return;
@@ -2682,12 +2821,7 @@ void llama_kv_self_seq_add(
 }
 
 // deprecated
-void llama_kv_self_seq_div(
-        llama_context * ctx,
-         llama_seq_id   seq_id,
-            llama_pos   p0,
-            llama_pos   p1,
-                  int   d) {
+void llama_kv_self_seq_div(llama_context * ctx, llama_seq_id seq_id, llama_pos p0, llama_pos p1, int d) {
     auto * kv = llama_get_memory(ctx);
     if (!kv) {
         return;
@@ -2750,12 +2884,19 @@ size_t llama_set_state_data(llama_context * ctx, const uint8_t * src) {
 }
 
 // deprecated
-bool llama_load_session_file(llama_context * ctx, const char * path_session, llama_token * tokens_out, size_t n_token_capacity, size_t * n_token_count_out) {
+bool llama_load_session_file(llama_context * ctx,
+                             const char *    path_session,
+                             llama_token *   tokens_out,
+                             size_t          n_token_capacity,
+                             size_t *        n_token_count_out) {
     return llama_state_load_file(ctx, path_session, tokens_out, n_token_capacity, n_token_count_out);
 }
 
 // deprecated
-bool llama_save_session_file(llama_context * ctx, const char * path_session, const llama_token * tokens, size_t n_token_count) {
+bool llama_save_session_file(llama_context *     ctx,
+                             const char *        path_session,
+                             const llama_token * tokens,
+                             size_t              n_token_count) {
     return llama_state_save_file(ctx, path_session, tokens, n_token_count);
 }
 
@@ -2778,7 +2919,11 @@ size_t llama_state_set_data(llama_context * ctx, const uint8_t * src, size_t siz
     return ctx->state_set_data(src, size);
 }
 
-bool llama_state_load_file(llama_context * ctx, const char * path_session, llama_token * tokens_out, size_t n_token_capacity, size_t * n_token_count_out) {
+bool llama_state_load_file(llama_context * ctx,
+                           const char *    path_session,
+                           llama_token *   tokens_out,
+                           size_t          n_token_capacity,
+                           size_t *        n_token_count_out) {
     ctx->synchronize();
 
     try {
@@ -2789,7 +2934,10 @@ bool llama_state_load_file(llama_context * ctx, const char * path_session, llama
     }
 }
 
-bool llama_state_save_file(llama_context * ctx, const char * path_session, const llama_token * tokens, size_t n_token_count) {
+bool llama_state_save_file(llama_context *     ctx,
+                           const char *        path_session,
+                           const llama_token * tokens,
+                           size_t              n_token_count) {
     ctx->synchronize();
 
     try {
@@ -2816,7 +2964,11 @@ size_t llama_state_seq_set_data(llama_context * ctx, const uint8_t * src, size_t
     return ctx->state_seq_set_data(seq_id, src, size);
 }
 
-size_t llama_state_seq_save_file(llama_context * ctx, const char * filepath, llama_seq_id seq_id, const llama_token * tokens, size_t n_token_count) {
+size_t llama_state_seq_save_file(llama_context *     ctx,
+                                 const char *        filepath,
+                                 llama_seq_id        seq_id,
+                                 const llama_token * tokens,
+                                 size_t              n_token_count) {
     ctx->synchronize();
 
     try {
@@ -2827,7 +2979,12 @@ size_t llama_state_seq_save_file(llama_context * ctx, const char * filepath, lla
     }
 }
 
-size_t llama_state_seq_load_file(llama_context * ctx, const char * filepath, llama_seq_id dest_seq_id, llama_token * tokens_out, size_t n_token_capacity, size_t * n_token_count_out) {
+size_t llama_state_seq_load_file(llama_context * ctx,
+                                 const char *    filepath,
+                                 llama_seq_id    dest_seq_id,
+                                 llama_token *   tokens_out,
+                                 size_t          n_token_capacity,
+                                 size_t *        n_token_count_out) {
     ctx->synchronize();
 
     try {
@@ -2840,9 +2997,7 @@ size_t llama_state_seq_load_file(llama_context * ctx, const char * filepath, lla
 
 ///
 
-int32_t llama_encode(
-        llama_context * ctx,
-          llama_batch   batch) {
+int32_t llama_encode(llama_context * ctx, llama_batch batch) {
     const int ret = ctx->encode(batch);
     if (ret != 0) {
         LLAMA_LOG_ERROR("%s: failed to encode, ret = %d\n", __func__, ret);
@@ -2851,9 +3006,7 @@ int32_t llama_encode(
     return ret;
 }
 
-int32_t llama_decode(
-        llama_context * ctx,
-          llama_batch   batch) {
+int32_t llama_decode(llama_context * ctx, llama_batch batch) {
     const int ret = ctx->decode(batch);
     if (ret != 0 && ret != 1) {
         LLAMA_LOG_ERROR("%s: failed to decode, ret = %d\n", __func__, ret);
@@ -2885,10 +3038,21 @@ void llama_perf_context_print(const llama_context * ctx) {
 
     LLAMA_LOG_INFO("%s:        load time = %10.2f ms\n", __func__, data.t_load_ms);
     LLAMA_LOG_INFO("%s: prompt eval time = %10.2f ms / %5d tokens (%8.2f ms per token, %8.2f tokens per second)\n",
-            __func__, data.t_p_eval_ms, data.n_p_eval, data.t_p_eval_ms / data.n_p_eval, 1e3 / data.t_p_eval_ms * data.n_p_eval);
+                   __func__,
+                   data.t_p_eval_ms,
+                   data.n_p_eval,
+                   data.t_p_eval_ms / data.n_p_eval,
+                   1e3 / data.t_p_eval_ms * data.n_p_eval);
     LLAMA_LOG_INFO("%s:        eval time = %10.2f ms / %5d runs   (%8.2f ms per token, %8.2f tokens per second)\n",
-            __func__, data.t_eval_ms, data.n_eval, data.t_eval_ms / data.n_eval, 1e3 / data.t_eval_ms * data.n_eval);
-    LLAMA_LOG_INFO("%s:       total time = %10.2f ms / %5d tokens\n", __func__, (t_end_ms - data.t_start_ms), (data.n_p_eval + data.n_eval));
+                   __func__,
+                   data.t_eval_ms,
+                   data.n_eval,
+                   data.t_eval_ms / data.n_eval,
+                   1e3 / data.t_eval_ms * data.n_eval);
+    LLAMA_LOG_INFO("%s:       total time = %10.2f ms / %5d tokens\n",
+                   __func__,
+                   (t_end_ms - data.t_start_ms),
+                   (data.n_p_eval + data.n_eval));
     LLAMA_LOG_INFO("%s:    graphs reused = %10d\n", __func__, data.n_reused);
 }
 
@@ -2910,19 +3074,12 @@ void llama_opt_init(struct llama_context * ctx, struct llama_model * model, stru
     ctx->opt_init(model, lopt_params);
 }
 
-void llama_opt_epoch(
-        struct llama_context    * ctx,
-        ggml_opt_dataset_t        dataset,
-        ggml_opt_result_t         result_train,
-        ggml_opt_result_t         result_eval,
-        int64_t                   idata_split,
-        ggml_opt_epoch_callback   callback_train,
-        ggml_opt_epoch_callback   callback_eval) {
-    ctx->opt_epoch(
-        dataset,
-        result_train,
-        result_eval,
-        idata_split,
-        callback_train,
-        callback_eval);
+void llama_opt_epoch(struct llama_context *  ctx,
+                     ggml_opt_dataset_t      dataset,
+                     ggml_opt_result_t       result_train,
+                     ggml_opt_result_t       result_eval,
+                     int64_t                 idata_split,
+                     ggml_opt_epoch_callback callback_train,
+                     ggml_opt_epoch_callback callback_eval) {
+    ctx->opt_epoch(dataset, result_train, result_eval, idata_split, callback_train, callback_eval);
 }
diff --git a/src/llama-graph.cpp b/src/llama-graph.cpp
index 053c72d6..a78b7507 100644
--- a/src/llama-graph.cpp
+++ b/src/llama-graph.cpp
@@ -1,11 +1,10 @@
 #include "llama-graph.h"
 
-#include "llama-impl.h"
 #include "llama-batch.h"
 #include "llama-cparams.h"
-
-#include "llama-kv-cache-unified.h"
+#include "llama-impl.h"
 #include "llama-kv-cache-unified-iswa.h"
+#include "llama-kv-cache-unified.h"
 #include "llama-memory-hybrid.h"
 #include "llama-memory-recurrent.h"
 
@@ -17,14 +16,14 @@ void llm_graph_input_embd::set_input(const llama_ubatch * ubatch) {
     if (ubatch->token) {
         const int64_t n_tokens = ubatch->n_tokens;
 
-        ggml_backend_tensor_set(tokens, ubatch->token, 0, n_tokens*ggml_element_size(tokens));
+        ggml_backend_tensor_set(tokens, ubatch->token, 0, n_tokens * ggml_element_size(tokens));
     }
 
     if (ubatch->embd) {
         const int64_t n_embd   = embd->ne[0];
         const int64_t n_tokens = ubatch->n_tokens;
 
-        ggml_backend_tensor_set(embd, ubatch->embd, 0, n_tokens*n_embd*ggml_element_size(embd));
+        ggml_backend_tensor_set(embd, ubatch->embd, 0, n_tokens * n_embd * ggml_element_size(embd));
     }
 }
 
@@ -32,7 +31,7 @@ bool llm_graph_input_embd::can_reuse(const llm_graph_params & params) {
     bool res = true;
 
     res &= (!tokens && !params.ubatch.token) || (tokens && tokens->ne[0] == params.ubatch.n_tokens);
-    res &= (!embd   && !params.ubatch.embd)  || (embd   &&   embd->ne[0] == params.ubatch.n_tokens);
+    res &= (!embd && !params.ubatch.embd) || (embd && embd->ne[0] == params.ubatch.n_tokens);
 
     return res;
 }
@@ -44,17 +43,17 @@ void llm_graph_input_pos::set_input(const llama_ubatch * ubatch) {
         if (ubatch->token && n_pos_per_embd == 4) {
             // in case we're using M-RoPE with text tokens, convert the 1D positions to 4D
             // the 3 first dims are the same, and 4th dim is all 0
-            std::vector<llama_pos> pos_data(n_tokens*n_pos_per_embd);
+            std::vector<llama_pos> pos_data(n_tokens * n_pos_per_embd);
             // copy the first dimension
             for (int i = 0; i < n_tokens; ++i) {
-                pos_data[               i] = ubatch->pos[i];
-                pos_data[    n_tokens + i] = ubatch->pos[i];
+                pos_data[i]                = ubatch->pos[i];
+                pos_data[n_tokens + i]     = ubatch->pos[i];
                 pos_data[2 * n_tokens + i] = ubatch->pos[i];
-                pos_data[3 * n_tokens + i] = 0; // 4th dim is 0
+                pos_data[3 * n_tokens + i] = 0;  // 4th dim is 0
             }
-            ggml_backend_tensor_set(pos, pos_data.data(), 0, pos_data.size()*ggml_element_size(pos));
+            ggml_backend_tensor_set(pos, pos_data.data(), 0, pos_data.size() * ggml_element_size(pos));
         } else {
-            ggml_backend_tensor_set(pos, ubatch->pos, 0, n_tokens*n_pos_per_embd*ggml_element_size(pos));
+            ggml_backend_tensor_set(pos, ubatch->pos, 0, n_tokens * n_pos_per_embd * ggml_element_size(pos));
         }
     }
 }
@@ -74,12 +73,11 @@ void llm_graph_input_attn_temp::set_input(const llama_ubatch * ubatch) {
         std::vector<float> attn_scale_data(n_tokens, 0.0f);
         for (int i = 0; i < n_tokens; ++i) {
             const float pos = ubatch->pos[i];
-            attn_scale_data[i] = std::log(
-                std::floor((pos + 1.0f) / n_attn_temp_floor_scale) + 1.0
-            ) * f_attn_temp_scale + 1.0;
+            attn_scale_data[i] =
+                std::log(std::floor((pos + 1.0f) / n_attn_temp_floor_scale) + 1.0) * f_attn_temp_scale + 1.0;
         }
 
-        ggml_backend_tensor_set(attn_scale, attn_scale_data.data(), 0, n_tokens*ggml_element_size(attn_scale));
+        ggml_backend_tensor_set(attn_scale, attn_scale_data.data(), 0, n_tokens * ggml_element_size(attn_scale));
     }
 }
 
@@ -88,14 +86,15 @@ void llm_graph_input_pos_bucket::set_input(const llama_ubatch * ubatch) {
         const int64_t n_tokens = ubatch->n_tokens;
 
         GGML_ASSERT(ggml_backend_buffer_is_host(pos_bucket->buffer));
-        GGML_ASSERT(!ubatch->equal_seqs()); // TODO: use ubatch->n_seqs instead of failing
+        GGML_ASSERT(!ubatch->equal_seqs());  // TODO: use ubatch->n_seqs instead of failing
 
         int32_t * data = (int32_t *) pos_bucket->data;
 
         for (int h = 0; h < 1; ++h) {
             for (int j = 0; j < n_tokens; ++j) {
                 for (int i = 0; i < n_tokens; ++i) {
-                    data[h*(n_tokens*n_tokens) + j*n_tokens + i] = llama_relative_position_bucket(ubatch->pos[i], ubatch->pos[j], hparams.n_rel_attn_bkts, true);
+                    data[h * (n_tokens * n_tokens) + j * n_tokens + i] =
+                        llama_relative_position_bucket(ubatch->pos[i], ubatch->pos[j], hparams.n_rel_attn_bkts, true);
                 }
             }
         }
@@ -149,57 +148,63 @@ void llm_graph_input_mean::set_input(const llama_ubatch * ubatch) {
         const int64_t n_seq_tokens = ubatch->n_seq_tokens;
         const int64_t n_seqs_unq   = ubatch->n_seqs_unq;
 
+        // Debug information removed for production
+
         GGML_ASSERT(mean);
         GGML_ASSERT(ggml_backend_buffer_is_host(mean->buffer));
 
         float * data = (float *) mean->data;
-        memset(mean->data, 0, n_tokens*n_seqs_unq*ggml_element_size(mean));
+        memset(mean->data, 0, n_tokens * n_seqs_unq * ggml_element_size(mean));
 
-        std::vector<uint64_t> sums(n_seqs_unq, 0);
-        for (int i = 0; i < n_tokens; i += n_seq_tokens) {
+        // For mean pooling, we need to ensure that all tokens in a sequence are processed together
+        // The current implementation assumes n_seq_tokens is constant, but this is not true when
+        // sequences are split across multiple ubatches. We need to fix this.
+
+        // Calculate the actual number of tokens per sequence in this ubatch
+        std::vector<uint64_t> actual_tokens_per_seq(n_seqs_unq, 0);
+        for (int i = 0; i < n_tokens; ++i) {
             for (int s = 0; s < ubatch->n_seq_id[i]; ++s) {
                 const llama_seq_id seq_id  = ubatch->seq_id[i][s];
                 const int32_t      seq_idx = ubatch->seq_idx[seq_id];
-
-                sums[seq_idx] += ubatch->n_seq_tokens;
+                actual_tokens_per_seq[seq_idx]++;
             }
         }
 
+        // Debug output removed for production
+
         std::vector<float> div(n_seqs_unq, 0.0f);
         for (int s = 0; s < n_seqs_unq; ++s) {
-            const uint64_t sum = sums[s];
-            if (sum > 0) {
-                div[s] = 1.0f/float(sum);
+            const uint64_t actual_tokens = actual_tokens_per_seq[s];
+            if (actual_tokens > 0) {
+                div[s] = 1.0f / float(actual_tokens);
             }
         }
 
-        for (int i = 0; i < n_tokens; i += n_seq_tokens) {
-            for (int s = 0; s < ubatch->n_seq_id[i]; ++s) {
-                const llama_seq_id seq_id  = ubatch->seq_id[i][s];
-                const int32_t      seq_idx = ubatch->seq_idx[seq_id];
+        // Debug output removed for production
 
-                for (int j = 0; j < n_seq_tokens; ++j) {
-                    data[seq_idx*n_tokens + i + j] = div[seq_idx];
-                }
+        // Apply the mean pooling weights
+        for (int i = 0; i < n_tokens; ++i) {
+            for (int s = 0; s < ubatch->n_seq_id[i]; ++s) {
+                const llama_seq_id seq_id    = ubatch->seq_id[i][s];
+                const int32_t      seq_idx   = ubatch->seq_idx[seq_id];
+                data[seq_idx * n_tokens + i] = div[seq_idx];
             }
         }
     }
 }
 
 void llm_graph_input_cls::set_input(const llama_ubatch * ubatch) {
-    const int64_t n_tokens     = ubatch->n_tokens;
-    const int64_t n_seqs_unq   = ubatch->n_seqs_unq;
-
-    if (cparams.embeddings && (
-        cparams.pooling_type == LLAMA_POOLING_TYPE_CLS  ||
-        cparams.pooling_type == LLAMA_POOLING_TYPE_RANK ||
-        cparams.pooling_type == LLAMA_POOLING_TYPE_LAST
-    )) {
+    const int64_t n_tokens   = ubatch->n_tokens;
+    const int64_t n_seqs_unq = ubatch->n_seqs_unq;
+
+    if (cparams.embeddings &&
+        (cparams.pooling_type == LLAMA_POOLING_TYPE_CLS || cparams.pooling_type == LLAMA_POOLING_TYPE_RANK ||
+         cparams.pooling_type == LLAMA_POOLING_TYPE_LAST)) {
         GGML_ASSERT(cls);
         GGML_ASSERT(ggml_backend_buffer_is_host(cls->buffer));
 
         uint32_t * data = (uint32_t *) cls->data;
-        memset(cls->data, 0, n_seqs_unq*ggml_element_size(cls));
+        memset(cls->data, 0, n_seqs_unq * ggml_element_size(cls));
 
         std::vector<int> target_pos(n_seqs_unq, -1);
         std::vector<int> target_row(n_seqs_unq, -1);
@@ -213,11 +218,8 @@ void llm_graph_input_cls::set_input(const llama_ubatch * ubatch) {
                 const llama_seq_id seq_id  = ubatch->seq_id[i][s];
                 const int32_t      seq_idx = ubatch->seq_idx[seq_id];
 
-                if (
-                    (target_pos[seq_idx] == -1) ||
-                    ( last && pos >= target_pos[seq_idx]) ||
-                    (!last && pos <  target_pos[seq_idx])
-                ) {
+                if ((target_pos[seq_idx] == -1) || (last && pos >= target_pos[seq_idx]) ||
+                    (!last && pos < target_pos[seq_idx])) {
                     target_pos[seq_idx] = pos;
                     target_row[seq_idx] = i;
                 }
@@ -288,7 +290,7 @@ void llm_graph_input_attn_no_cache::set_input(const llama_ubatch * ubatch) {
                     }
                 }
 
-                data[h*(n_kv*n_tokens) + i1*n_kv + i0] = f;
+                data[h * (n_kv * n_tokens) + i1 * n_kv + i0] = f;
             }
         }
     }
@@ -309,12 +311,12 @@ bool llm_graph_input_attn_kv_unified::can_reuse(const llm_graph_params & params)
     bool res = true;
 
     res &= self_k_idxs->ne[0] == params.ubatch.n_tokens;
-  //res &= self_v_idxs->ne[0] == params.ubatch.n_tokens; // TODO: need to move this to the unified cache and check there
+    //res &= self_v_idxs->ne[0] == params.ubatch.n_tokens; // TODO: need to move this to the unified cache and check there
 
     res &= self_kq_mask->ne[0] == mctx->get_n_kv();
     res &= self_kq_mask->ne[1] == GGML_PAD(params.ubatch.n_tokens, GGML_KQ_MASK_PAD);
 
-    res &= mctx->get_supports_set_rows(); // TODO: tmp
+    res &= mctx->get_supports_set_rows();  // TODO: tmp
 
     return res;
 }
@@ -339,10 +341,10 @@ bool llm_graph_input_attn_kv_unified_iswa::can_reuse(const llm_graph_params & pa
     bool res = true;
 
     res &= self_k_idxs->ne[0] == params.ubatch.n_tokens;
-  //res &= self_v_idxs->ne[0] == params.ubatch.n_tokens; // TODO: need to move this to the unified cache and check there
+    //res &= self_v_idxs->ne[0] == params.ubatch.n_tokens; // TODO: need to move this to the unified cache and check there
 
     res &= self_k_idxs_swa->ne[0] == params.ubatch.n_tokens;
-  //res &= self_v_idxs_swa->ne[0] == params.ubatch.n_tokens; // TODO: need to move this to the unified cache and check there
+    //res &= self_v_idxs_swa->ne[0] == params.ubatch.n_tokens; // TODO: need to move this to the unified cache and check there
 
     res &= self_kq_mask->ne[0] == mctx->get_base()->get_n_kv();
     res &= self_kq_mask->ne[1] == GGML_PAD(params.ubatch.n_tokens, GGML_KQ_MASK_PAD);
@@ -350,7 +352,7 @@ bool llm_graph_input_attn_kv_unified_iswa::can_reuse(const llm_graph_params & pa
     res &= self_kq_mask_swa->ne[0] == mctx->get_swa()->get_n_kv();
     res &= self_kq_mask_swa->ne[1] == GGML_PAD(params.ubatch.n_tokens, GGML_KQ_MASK_PAD);
 
-    res &= mctx->get_base()->get_supports_set_rows(); // TODO: tmp
+    res &= mctx->get_base()->get_supports_set_rows();  // TODO: tmp
 
     return res;
 }
@@ -362,7 +364,7 @@ void llm_graph_input_attn_cross::set_input(const llama_ubatch * ubatch) {
     const int64_t n_tokens = ubatch->n_tokens;
 
     GGML_ASSERT(ggml_backend_buffer_is_host(cross_kq_mask->buffer));
-    GGML_ASSERT(!ubatch->equal_seqs()); // TODO: use ubatch->n_seqs instead of failing
+    GGML_ASSERT(!ubatch->equal_seqs());  // TODO: use ubatch->n_seqs instead of failing
 
     float * data = (float *) cross_kq_mask->data;
 
@@ -379,13 +381,13 @@ void llm_graph_input_attn_cross::set_input(const llama_ubatch * ubatch) {
                     }
                 }
 
-                data[h*(n_enc*n_tokens) + i*n_enc + j] = f;
+                data[h * (n_enc * n_tokens) + i * n_enc + j] = f;
             }
         }
 
         for (int i = n_tokens; i < GGML_PAD(n_tokens, GGML_KQ_MASK_PAD); ++i) {
             for (int j = 0; j < n_enc; ++j) {
-                data[h*(n_enc*n_tokens) + i*n_enc + j] = -INFINITY;
+                data[h * (n_enc * n_tokens) + i * n_enc + j] = -INFINITY;
             }
         }
     }
@@ -404,7 +406,7 @@ llm_graph_result::llm_graph_result(int64_t max_nodes) : max_nodes(max_nodes) {
     reset();
 
     const char * LLAMA_GRAPH_RESULT_DEBUG = getenv("LLAMA_GRAPH_RESULT_DEBUG");
-    debug = LLAMA_GRAPH_RESULT_DEBUG ? atoi(LLAMA_GRAPH_RESULT_DEBUG) : 0;
+    debug                                 = LLAMA_GRAPH_RESULT_DEBUG ? atoi(LLAMA_GRAPH_RESULT_DEBUG) : 0;
 }
 
 int64_t llm_graph_result::get_max_nodes() const {
@@ -421,12 +423,12 @@ void llm_graph_result::reset() {
 
     inputs.clear();
 
-    buf_compute_meta.resize(ggml_tensor_overhead()*max_nodes + ggml_graph_overhead_custom(max_nodes, false));
+    buf_compute_meta.resize(ggml_tensor_overhead() * max_nodes + ggml_graph_overhead_custom(max_nodes, false));
 
     ggml_init_params params = {
-        /*.mem_size   =*/ buf_compute_meta.size(),
-        /*.mem_buffer =*/ buf_compute_meta.data(),
-        /*.no_alloc   =*/ true,
+        /*.mem_size   =*/buf_compute_meta.size(),
+        /*.mem_buffer =*/buf_compute_meta.data(),
+        /*.no_alloc   =*/true,
     };
 
     ctx_compute.reset(ggml_init(params));
@@ -486,47 +488,47 @@ void llm_graph_result::set_params(const llm_graph_params & params) {
 //
 
 llm_graph_context::llm_graph_context(const llm_graph_params & params) :
-    arch             (params.arch),
-    hparams          (params.hparams),
-    cparams          (params.cparams),
-    ubatch           (params.ubatch),
-    n_embd           (hparams.n_embd),
-    n_layer          (hparams.n_layer),
-    n_rot            (hparams.n_rot),
-    n_ctx            (cparams.n_ctx),
-    n_head           (hparams.n_head()),
-    n_head_kv        (hparams.n_head_kv()),
-    n_embd_head_k    (hparams.n_embd_head_k),
-    n_embd_k_gqa     (hparams.n_embd_k_gqa()),
-    n_embd_head_v    (hparams.n_embd_head_v),
-    n_embd_v_gqa     (hparams.n_embd_v_gqa()),
-    n_expert         (hparams.n_expert),
-    n_expert_used    (cparams.warmup ? hparams.n_expert : hparams.n_expert_used),
-    freq_base        (cparams.rope_freq_base),
-    freq_scale       (cparams.rope_freq_scale),
-    ext_factor       (cparams.yarn_ext_factor),
-    attn_factor      (cparams.yarn_attn_factor),
-    beta_fast        (cparams.yarn_beta_fast),
-    beta_slow        (cparams.yarn_beta_slow),
-    norm_eps         (hparams.f_norm_eps),
-    norm_rms_eps     (hparams.f_norm_rms_eps),
-    n_tokens         (ubatch.n_tokens),
-    n_outputs        (params.n_outputs),
-    n_ctx_orig       (cparams.n_ctx_orig_yarn),
-    pooling_type     (cparams.pooling_type),
-    rope_type        (hparams.rope_type),
-    sched            (params.sched),
-    backend_cpu      (params.backend_cpu),
-    cvec             (params.cvec),
-    loras            (params.loras),
-    mctx             (params.mctx),
-    cross            (params.cross),
-    cb_func          (params.cb),
-    res              (params.res),
-    ctx0             (res->get_ctx()),
-    gf               (res->get_gf()) {
-        res->set_params(params);
-    }
+    arch(params.arch),
+    hparams(params.hparams),
+    cparams(params.cparams),
+    ubatch(params.ubatch),
+    n_embd(hparams.n_embd),
+    n_layer(hparams.n_layer),
+    n_rot(hparams.n_rot),
+    n_ctx(cparams.n_ctx),
+    n_head(hparams.n_head()),
+    n_head_kv(hparams.n_head_kv()),
+    n_embd_head_k(hparams.n_embd_head_k),
+    n_embd_k_gqa(hparams.n_embd_k_gqa()),
+    n_embd_head_v(hparams.n_embd_head_v),
+    n_embd_v_gqa(hparams.n_embd_v_gqa()),
+    n_expert(hparams.n_expert),
+    n_expert_used(cparams.warmup ? hparams.n_expert : hparams.n_expert_used),
+    freq_base(cparams.rope_freq_base),
+    freq_scale(cparams.rope_freq_scale),
+    ext_factor(cparams.yarn_ext_factor),
+    attn_factor(cparams.yarn_attn_factor),
+    beta_fast(cparams.yarn_beta_fast),
+    beta_slow(cparams.yarn_beta_slow),
+    norm_eps(hparams.f_norm_eps),
+    norm_rms_eps(hparams.f_norm_rms_eps),
+    n_tokens(ubatch.n_tokens),
+    n_outputs(params.n_outputs),
+    n_ctx_orig(cparams.n_ctx_orig_yarn),
+    pooling_type(cparams.pooling_type),
+    rope_type(hparams.rope_type),
+    sched(params.sched),
+    backend_cpu(params.backend_cpu),
+    cvec(params.cvec),
+    loras(params.loras),
+    mctx(params.mctx),
+    cross(params.cross),
+    cb_func(params.cb),
+    res(params.res),
+    ctx0(res->get_ctx()),
+    gf(res->get_gf()) {
+    res->set_params(params);
+}
 
 void llm_graph_context::cb(ggml_tensor * cur, const char * name, int il) const {
     if (cb_func) {
@@ -534,15 +536,11 @@ void llm_graph_context::cb(ggml_tensor * cur, const char * name, int il) const {
     }
 }
 
-ggml_tensor * llm_graph_context::build_cvec(
-         ggml_tensor * cur,
-                 int   il) const {
+ggml_tensor * llm_graph_context::build_cvec(ggml_tensor * cur, int il) const {
     return cvec->apply_to(ctx0, cur, il);
 }
 
-ggml_tensor * llm_graph_context::build_lora_mm(
-          ggml_tensor * w,
-          ggml_tensor * cur) const {
+ggml_tensor * llm_graph_context::build_lora_mm(ggml_tensor * w, ggml_tensor * cur) const {
     ggml_tensor * res = ggml_mul_mat(ctx0, w, cur);
 
     for (const auto & lora : *loras) {
@@ -552,24 +550,20 @@ ggml_tensor * llm_graph_context::build_lora_mm(
         }
 
         const float adapter_scale = lora.second;
-        const float scale = lw->get_scale(lora.first->alpha, adapter_scale);
+        const float scale         = lw->get_scale(lora.first->alpha, adapter_scale);
 
-        ggml_tensor * ab_cur = ggml_mul_mat(
-                ctx0, lw->b,
-                ggml_mul_mat(ctx0, lw->a, cur)
-                );
+        ggml_tensor * ab_cur = ggml_mul_mat(ctx0, lw->b, ggml_mul_mat(ctx0, lw->a, cur));
 
         ab_cur = ggml_scale(ctx0, ab_cur, scale);
-        res = ggml_add(ctx0, res, ab_cur);
+        res    = ggml_add(ctx0, res, ab_cur);
     }
 
     return res;
 }
 
-ggml_tensor * llm_graph_context::build_lora_mm_id(
-          ggml_tensor * w,   // ggml_tensor * as
-          ggml_tensor * cur, // ggml_tensor * b
-          ggml_tensor * ids) const {
+ggml_tensor * llm_graph_context::build_lora_mm_id(ggml_tensor * w,    // ggml_tensor * as
+                                                  ggml_tensor * cur,  // ggml_tensor * b
+                                                  ggml_tensor * ids) const {
     ggml_tensor * res = ggml_mul_mat_id(ctx0, w, cur, ids);
     for (const auto & lora : *loras) {
         llama_adapter_lora_weight * lw = lora.first->get_weight(w);
@@ -581,34 +575,34 @@ ggml_tensor * llm_graph_context::build_lora_mm_id(
         const float rank  = (float) lw->b->ne[0];
         const float scale = alpha ? lora.second * alpha / rank : lora.second;
 
-        ggml_tensor * ab_cur = ggml_mul_mat_id(
-                ctx0, lw->b,
-                ggml_mul_mat_id(ctx0, lw->a, cur, ids),
-                ids
-                );
+        ggml_tensor * ab_cur = ggml_mul_mat_id(ctx0, lw->b, ggml_mul_mat_id(ctx0, lw->a, cur, ids), ids);
 
         ab_cur = ggml_scale(ctx0, ab_cur, scale);
-        res = ggml_add(ctx0, res, ab_cur);
+        res    = ggml_add(ctx0, res, ab_cur);
     }
 
     return res;
 }
 
-ggml_tensor * llm_graph_context::build_norm(
-         ggml_tensor * cur,
-         ggml_tensor * mw,
-         ggml_tensor * mb,
-       llm_norm_type   type,
-                 int   il) const {
+ggml_tensor * llm_graph_context::build_norm(ggml_tensor * cur,
+                                            ggml_tensor * mw,
+                                            ggml_tensor * mb,
+                                            llm_norm_type type,
+                                            int           il) const {
     switch (type) {
-        case LLM_NORM:       cur = ggml_norm    (ctx0, cur, hparams.f_norm_eps);     break;
-        case LLM_NORM_RMS:   cur = ggml_rms_norm(ctx0, cur, hparams.f_norm_rms_eps); break;
+        case LLM_NORM:
+            cur = ggml_norm(ctx0, cur, hparams.f_norm_eps);
+            break;
+        case LLM_NORM_RMS:
+            cur = ggml_rms_norm(ctx0, cur, hparams.f_norm_rms_eps);
+            break;
         case LLM_NORM_GROUP:
             {
                 cur = ggml_reshape_3d(ctx0, cur, cur->ne[0], 1, cur->ne[1]);
                 cur = ggml_group_norm(ctx0, cur, hparams.n_norm_groups, hparams.f_norm_group_eps);
-                cur = ggml_reshape_2d(ctx0, cur, cur->ne[0],    cur->ne[2]);
-            } break;
+                cur = ggml_reshape_2d(ctx0, cur, cur->ne[0], cur->ne[2]);
+            }
+            break;
     }
 
     if (mw || mb) {
@@ -629,21 +623,20 @@ ggml_tensor * llm_graph_context::build_norm(
     return cur;
 }
 
-ggml_tensor * llm_graph_context::build_ffn(
-         ggml_tensor * cur,
-         ggml_tensor * up,
-         ggml_tensor * up_b,
-         ggml_tensor * up_s,
-         ggml_tensor * gate,
-         ggml_tensor * gate_b,
-         ggml_tensor * gate_s,
-         ggml_tensor * down,
-         ggml_tensor * down_b,
-         ggml_tensor * down_s,
-         ggml_tensor * act_scales,
-     llm_ffn_op_type   type_op,
-   llm_ffn_gate_type   type_gate,
-                 int   il) const {
+ggml_tensor * llm_graph_context::build_ffn(ggml_tensor *     cur,
+                                           ggml_tensor *     up,
+                                           ggml_tensor *     up_b,
+                                           ggml_tensor *     up_s,
+                                           ggml_tensor *     gate,
+                                           ggml_tensor *     gate_b,
+                                           ggml_tensor *     gate_s,
+                                           ggml_tensor *     down,
+                                           ggml_tensor *     down_b,
+                                           ggml_tensor *     down_s,
+                                           ggml_tensor *     act_scales,
+                                           llm_ffn_op_type   type_op,
+                                           llm_ffn_gate_type type_gate,
+                                           int               il) const {
     ggml_tensor * tmp = up ? build_lora_mm(up, cur) : cur;
     cb(tmp, "ffn_up", il);
 
@@ -663,12 +656,14 @@ ggml_tensor * llm_graph_context::build_ffn(
                 {
                     cur = build_lora_mm(gate, tmp);
                     cb(cur, "ffn_gate", il);
-                } break;
+                }
+                break;
             case LLM_FFN_PAR:
                 {
                     cur = build_lora_mm(gate, cur);
                     cb(cur, "ffn_gate", il);
-                } break;
+                }
+                break;
         }
 
         if (gate_b) {
@@ -694,7 +689,8 @@ ggml_tensor * llm_graph_context::build_ffn(
             } else {
                 cur = ggml_silu(ctx0, cur);
                 cb(cur, "ffn_silu", il);
-            } break;
+            }
+            break;
         case LLM_FFN_GELU:
             if (gate && type_gate == LLM_FFN_PAR) {
                 cur = ggml_geglu_split(ctx0, cur, tmp);
@@ -707,7 +703,8 @@ ggml_tensor * llm_graph_context::build_ffn(
                     cur = ggml_div(ctx0, cur, act_scales);
                     cb(cur, "ffn_act", il);
                 }
-            } break;
+            }
+            break;
         case LLM_FFN_RELU:
             if (gate && type_gate == LLM_FFN_PAR) {
                 cur = ggml_reglu_split(ctx0, cur, tmp);
@@ -716,7 +713,8 @@ ggml_tensor * llm_graph_context::build_ffn(
             } else {
                 cur = ggml_relu(ctx0, cur);
                 cb(cur, "ffn_relu", il);
-            } break;
+            }
+            break;
         case LLM_FFN_RELU_SQR:
             {
                 cur = ggml_relu(ctx0, cur);
@@ -724,22 +722,26 @@ ggml_tensor * llm_graph_context::build_ffn(
 
                 cur = ggml_sqr(ctx0, cur);
                 cb(cur, "ffn_sqr(relu)", il);
-            } break;
+            }
+            break;
         case LLM_FFN_SWIGLU:
             {
                 cur = ggml_swiglu(ctx0, cur);
                 cb(cur, "ffn_swiglu", il);
-            } break;
+            }
+            break;
         case LLM_FFN_GEGLU:
             {
                 cur = ggml_geglu(ctx0, cur);
                 cb(cur, "ffn_geglu", il);
-            } break;
+            }
+            break;
         case LLM_FFN_REGLU:
             {
                 cur = ggml_reglu(ctx0, cur);
                 cb(cur, "ffn_reglu", il);
-            } break;
+            }
+            break;
         default:
             GGML_ABORT("fatal error");
     }
@@ -773,69 +775,70 @@ ggml_tensor * llm_graph_context::build_ffn(
     return cur;
 }
 
-ggml_tensor * llm_graph_context::build_moe_ffn(
-         ggml_tensor * cur,
-         ggml_tensor * gate_inp,
-         ggml_tensor * up_exps,
-         ggml_tensor * gate_exps,
-         ggml_tensor * down_exps,
-         ggml_tensor * exp_probs_b,
-             int64_t   n_expert,
-             int64_t   n_expert_used,
-     llm_ffn_op_type   type_op,
-                bool   norm_w,
-                bool   scale_w,
-               float   w_scale,
-         llama_expert_gating_func_type gating_op,
-                 int   il,
-         ggml_tensor * probs_in) const {
-    return build_moe_ffn(
-        cur,
-        gate_inp,  /* gate_inp_b  */ nullptr,
-        up_exps,   /* up_exps_b   */ nullptr,
-        gate_exps, /* gate_exps_b */ nullptr,
-        down_exps, /* down_exps_b */ nullptr,
-        exp_probs_b,
-        n_expert,
-        n_expert_used,
-        type_op,
-        norm_w,
-        scale_w,
-        w_scale,
-        gating_op,
-        il,
-        probs_in
-    );
-}
-
-ggml_tensor * llm_graph_context::build_moe_ffn(
-         ggml_tensor * cur,
-         ggml_tensor * gate_inp,
-         ggml_tensor * gate_inp_b,
-         ggml_tensor * up_exps,
-         ggml_tensor * up_exps_b,
-         ggml_tensor * gate_exps,
-         ggml_tensor * gate_exps_b,
-         ggml_tensor * down_exps,
-         ggml_tensor * down_exps_b,
-         ggml_tensor * exp_probs_b,
-             int64_t   n_expert,
-             int64_t   n_expert_used,
-     llm_ffn_op_type   type_op,
-                bool   norm_w,
-                bool   scale_w,
-               float   w_scale,
-        llama_expert_gating_func_type gating_op,
-                 int   il,
-         ggml_tensor * probs_in) const {
+ggml_tensor * llm_graph_context::build_moe_ffn(ggml_tensor *                 cur,
+                                               ggml_tensor *                 gate_inp,
+                                               ggml_tensor *                 up_exps,
+                                               ggml_tensor *                 gate_exps,
+                                               ggml_tensor *                 down_exps,
+                                               ggml_tensor *                 exp_probs_b,
+                                               int64_t                       n_expert,
+                                               int64_t                       n_expert_used,
+                                               llm_ffn_op_type               type_op,
+                                               bool                          norm_w,
+                                               bool                          scale_w,
+                                               float                         w_scale,
+                                               llama_expert_gating_func_type gating_op,
+                                               int                           il,
+                                               ggml_tensor *                 probs_in) const {
+    return build_moe_ffn(cur,
+                         gate_inp,
+                         /* gate_inp_b  */ nullptr,
+                         up_exps,
+                         /* up_exps_b   */ nullptr,
+                         gate_exps,
+                         /* gate_exps_b */ nullptr,
+                         down_exps,
+                         /* down_exps_b */ nullptr,
+                         exp_probs_b,
+                         n_expert,
+                         n_expert_used,
+                         type_op,
+                         norm_w,
+                         scale_w,
+                         w_scale,
+                         gating_op,
+                         il,
+                         probs_in);
+}
+
+ggml_tensor * llm_graph_context::build_moe_ffn(ggml_tensor *                 cur,
+                                               ggml_tensor *                 gate_inp,
+                                               ggml_tensor *                 gate_inp_b,
+                                               ggml_tensor *                 up_exps,
+                                               ggml_tensor *                 up_exps_b,
+                                               ggml_tensor *                 gate_exps,
+                                               ggml_tensor *                 gate_exps_b,
+                                               ggml_tensor *                 down_exps,
+                                               ggml_tensor *                 down_exps_b,
+                                               ggml_tensor *                 exp_probs_b,
+                                               int64_t                       n_expert,
+                                               int64_t                       n_expert_used,
+                                               llm_ffn_op_type               type_op,
+                                               bool                          norm_w,
+                                               bool                          scale_w,
+                                               float                         w_scale,
+                                               llama_expert_gating_func_type gating_op,
+                                               int                           il,
+                                               ggml_tensor *                 probs_in) const {
     const int64_t n_embd   = cur->ne[0];
     const int64_t n_tokens = cur->ne[1];
-    const bool weight_before_ffn = arch == LLM_ARCH_LLAMA4; // for llama4, we apply the sigmoid-ed weights before the FFN
+    const bool    weight_before_ffn =
+        arch == LLM_ARCH_LLAMA4;  // for llama4, we apply the sigmoid-ed weights before the FFN
 
     ggml_tensor * logits = nullptr;
 
     if (probs_in == nullptr) {
-        logits = build_lora_mm(gate_inp, cur); // [n_expert, n_tokens]
+        logits = build_lora_mm(gate_inp, cur);  // [n_expert, n_tokens]
         cb(logits, "ffn_moe_logits", il);
     } else {
         logits = probs_in;
@@ -850,16 +853,19 @@ ggml_tensor * llm_graph_context::build_moe_ffn(
     switch (gating_op) {
         case LLAMA_EXPERT_GATING_FUNC_TYPE_SOFTMAX:
             {
-                probs = ggml_soft_max(ctx0, logits); // [n_expert, n_tokens]
-            } break;
+                probs = ggml_soft_max(ctx0, logits);  // [n_expert, n_tokens]
+            }
+            break;
         case LLAMA_EXPERT_GATING_FUNC_TYPE_SIGMOID:
             {
-                probs = ggml_sigmoid(ctx0, logits); // [n_expert, n_tokens]
-            } break;
+                probs = ggml_sigmoid(ctx0, logits);  // [n_expert, n_tokens]
+            }
+            break;
         case LLAMA_EXPERT_GATING_FUNC_TYPE_SOFTMAX_WEIGHT:
             {
-                probs = logits; // [n_expert, n_tokens]
-            } break;
+                probs = logits;  // [n_expert, n_tokens]
+            }
+            break;
         default:
             GGML_ABORT("fatal error");
     }
@@ -880,17 +886,17 @@ ggml_tensor * llm_graph_context::build_moe_ffn(
     }
 
     // select experts
-    ggml_tensor * selected_experts = ggml_top_k(ctx0, selection_probs, n_expert_used); // [n_expert_used, n_tokens]
+    ggml_tensor * selected_experts = ggml_top_k(ctx0, selection_probs, n_expert_used);  // [n_expert_used, n_tokens]
     cb(selected_experts->src[0], "ffn_moe_argsort", il);
     cb(selected_experts, "ffn_moe_topk", il);
 
-    ggml_tensor * weights = ggml_get_rows(ctx0,
-            ggml_reshape_3d(ctx0, probs, 1, n_expert, n_tokens), selected_experts); // [1, n_expert_used, n_tokens]
+    ggml_tensor * weights = ggml_get_rows(
+        ctx0, ggml_reshape_3d(ctx0, probs, 1, n_expert, n_tokens), selected_experts);  // [1, n_expert_used, n_tokens]
     cb(weights, "ffn_moe_weights", il);
 
     if (gating_op == LLAMA_EXPERT_GATING_FUNC_TYPE_SOFTMAX_WEIGHT) {
         weights = ggml_reshape_2d(ctx0, weights, n_expert_used, n_tokens);
-        weights = ggml_soft_max(ctx0, weights); // [n_expert_used, n_tokens]
+        weights = ggml_soft_max(ctx0, weights);  // [n_expert_used, n_tokens]
         weights = ggml_reshape_3d(ctx0, weights, 1, n_expert_used, n_tokens);
         cb(weights, "ffn_moe_weights_softmax", il);
     }
@@ -898,10 +904,10 @@ ggml_tensor * llm_graph_context::build_moe_ffn(
     if (norm_w) {
         weights = ggml_reshape_2d(ctx0, weights, n_expert_used, n_tokens);
 
-        ggml_tensor * weights_sum = ggml_sum_rows(ctx0, weights); // [1, n_tokens]
+        ggml_tensor * weights_sum = ggml_sum_rows(ctx0, weights);  // [1, n_tokens]
         cb(weights_sum, "ffn_moe_weights_sum", il);
 
-        weights = ggml_div(ctx0, weights, weights_sum); // [n_expert_used, n_tokens]
+        weights = ggml_div(ctx0, weights, weights_sum);  // [n_expert_used, n_tokens]
         cb(weights, "ffn_moe_weights_norm", il);
 
         weights = ggml_reshape_3d(ctx0, weights, 1, n_expert_used, n_tokens);
@@ -916,11 +922,11 @@ ggml_tensor * llm_graph_context::build_moe_ffn(
     if (weight_before_ffn) {
         // repeat cur to [n_embd, n_expert_used, n_tokens]
         ggml_tensor * repeated = ggml_repeat_4d(ctx0, cur, n_embd, n_expert_used, n_tokens, 1);
-        cur = ggml_mul(ctx0, repeated, weights);
+        cur                    = ggml_mul(ctx0, repeated, weights);
         cb(cur, "ffn_moe_weighted", il);
     }
 
-    ggml_tensor * up = build_lora_mm_id(up_exps, cur, selected_experts); // [n_ff, n_expert_used, n_tokens]
+    ggml_tensor * up = build_lora_mm_id(up_exps, cur, selected_experts);  // [n_ff, n_expert_used, n_tokens]
     cb(up, "ffn_moe_up", il);
 
     if (up_exps_b) {
@@ -930,7 +936,7 @@ ggml_tensor * llm_graph_context::build_moe_ffn(
 
     ggml_tensor * experts = nullptr;
     if (gate_exps) {
-        cur = build_lora_mm_id(gate_exps, cur, selected_experts); // [n_ff, n_expert_used, n_tokens]
+        cur = build_lora_mm_id(gate_exps, cur, selected_experts);  // [n_ff, n_expert_used, n_tokens]
         cb(cur, "ffn_moe_gate", il);
     } else {
         cur = up;
@@ -949,7 +955,8 @@ ggml_tensor * llm_graph_context::build_moe_ffn(
             } else {
                 cur = ggml_silu(ctx0, cur);
                 cb(cur, "ffn_moe_silu", il);
-            } break;
+            }
+            break;
         case LLM_FFN_GELU:
             if (gate_exps) {
                 cur = ggml_geglu_split(ctx0, cur, up);
@@ -957,15 +964,17 @@ ggml_tensor * llm_graph_context::build_moe_ffn(
             } else {
                 cur = ggml_gelu(ctx0, cur);
                 cb(cur, "ffn_moe_gelu", il);
-            } break;
+            }
+            break;
         case LLM_FFN_SWIGLU_OAI_MOE:
             {
                 // TODO: move to hparams?
                 constexpr float alpha = 1.702f;
                 constexpr float limit = 7.0f;
-                cur = ggml_swiglu_oai(ctx0, cur, up, alpha, limit);
+                cur                   = ggml_swiglu_oai(ctx0, cur, up, alpha, limit);
                 cb(cur, "ffn_moe_swiglu_oai", il);
-            } break;
+            }
+            break;
         case LLM_FFN_RELU:
             if (gate_exps) {
                 cur = ggml_reglu_split(ctx0, cur, up);
@@ -973,12 +982,13 @@ ggml_tensor * llm_graph_context::build_moe_ffn(
             } else {
                 cur = ggml_relu(ctx0, cur);
                 cb(cur, "ffn_moe_relu", il);
-            } break;
+            }
+            break;
         default:
             GGML_ABORT("fatal error");
     }
 
-    experts = build_lora_mm_id(down_exps, cur, selected_experts); // [n_embd, n_expert_used, n_tokens]
+    experts = build_lora_mm_id(down_exps, cur, selected_experts);  // [n_embd, n_expert_used, n_tokens]
     cb(experts, "ffn_moe_down", il);
 
     if (down_exps_b) {
@@ -997,7 +1007,7 @@ ggml_tensor * llm_graph_context::build_moe_ffn(
 
     // order the views before the adds
     for (uint32_t i = 0; i < hparams.n_expert_used; ++i) {
-        cur_experts[i] = ggml_view_2d(ctx0, experts, n_embd, n_tokens, experts->nb[2], i*experts->nb[1]);
+        cur_experts[i] = ggml_view_2d(ctx0, experts, n_embd, n_tokens, experts->nb[2], i * experts->nb[1]);
 
         ggml_build_forward_expand(gf, cur_experts[i]);
     }
@@ -1046,12 +1056,13 @@ ggml_tensor * llm_graph_context::build_inp_embd(ggml_tensor * tok_embd) const {
             }
 
             const float adapter_scale = lora.second;
-            const float scale = lw->get_scale(lora.first->alpha, adapter_scale);
+            const float scale         = lw->get_scale(lora.first->alpha, adapter_scale);
 
-            ggml_tensor * inpL_delta = ggml_scale(ctx0, ggml_mul_mat(
-                        ctx0, lw->b, // non-transposed lora_b
-                        ggml_get_rows(ctx0, lw->a, inp->tokens)
-                        ), scale);
+            ggml_tensor * inpL_delta = ggml_scale(ctx0,
+                                                  ggml_mul_mat(ctx0,
+                                                               lw->b,  // non-transposed lora_b
+                                                               ggml_get_rows(ctx0, lw->a, inp->tokens)),
+                                                  scale);
 
             cur = ggml_add(ctx0, cur, inpL_delta);
         }
@@ -1079,7 +1090,7 @@ ggml_tensor * llm_graph_context::build_inp_pos() const {
 
     auto & cur = inp->pos;
 
-    cur = ggml_new_tensor_1d(ctx0, GGML_TYPE_I32, (int64_t)n_tokens*hparams.n_pos_per_embd());
+    cur = ggml_new_tensor_1d(ctx0, GGML_TYPE_I32, (int64_t) n_tokens * hparams.n_pos_per_embd());
     ggml_set_input(cur);
 
     res->add_input(std::move(inp));
@@ -1209,29 +1220,28 @@ ggml_tensor * llm_graph_context::build_pos_bias(ggml_tensor * pos_bucket, ggml_t
     ggml_tensor * pos_bias = ggml_get_rows(ctx0, attn_rel_b, pos_bucket_1d);
 
     pos_bias = ggml_reshape_3d(ctx0, pos_bias, pos_bias->ne[0], pos_bucket->ne[0], pos_bucket->ne[1]);
-    pos_bias = ggml_permute   (ctx0, pos_bias, 2, 0, 1, 3);
-    pos_bias = ggml_cont      (ctx0, pos_bias);
+    pos_bias = ggml_permute(ctx0, pos_bias, 2, 0, 1, 3);
+    pos_bias = ggml_cont(ctx0, pos_bias);
 
     cb(pos_bias, "pos_bias", -1);
 
     return pos_bias;
 }
 
-ggml_tensor * llm_graph_context::build_attn_mha(
-         ggml_tensor * q,
-         ggml_tensor * k,
-         ggml_tensor * v,
-         ggml_tensor * kq_b,
-         ggml_tensor * kq_mask,
-         ggml_tensor * v_mla,
-         ggml_tensor * sinks,
-             float     kq_scale) const {
+ggml_tensor * llm_graph_context::build_attn_mha(ggml_tensor * q,
+                                                ggml_tensor * k,
+                                                ggml_tensor * v,
+                                                ggml_tensor * kq_b,
+                                                ggml_tensor * kq_mask,
+                                                ggml_tensor * v_mla,
+                                                ggml_tensor * sinks,
+                                                float         kq_scale) const {
     const bool v_trans = v->nb[1] > v->nb[2];
 
     // split the batch into streams if needed
     const auto n_stream = k->ne[3];
 
-    q = ggml_reshape_4d(ctx0, q, q->ne[0], q->ne[1], q->ne[2]/n_stream, n_stream);
+    q = ggml_reshape_4d(ctx0, q, q->ne[0], q->ne[1], q->ne[2] / n_stream, n_stream);
 
     q = ggml_permute(ctx0, q, 0, 2, 1, 3);
     k = ggml_permute(ctx0, k, 0, 2, 1, 3);
@@ -1258,11 +1268,17 @@ ggml_tensor * llm_graph_context::build_attn_mha(
             v = ggml_cast(ctx0, v, GGML_TYPE_F16);
         }
 
-        cur = ggml_flash_attn_ext(ctx0, q, k, v, kq_mask, kq_scale, hparams.f_max_alibi_bias,
+        cur = ggml_flash_attn_ext(ctx0,
+                                  q,
+                                  k,
+                                  v,
+                                  kq_mask,
+                                  kq_scale,
+                                  hparams.f_max_alibi_bias,
                                   hparams.attn_soft_cap ? hparams.f_attn_logit_softcapping : 0.0f);
 
         ggml_flash_attn_ext_add_sinks(cur, sinks);
-        ggml_flash_attn_ext_set_prec (cur, GGML_PREC_F32);
+        ggml_flash_attn_ext_set_prec(cur, GGML_PREC_F32);
 
         if (v_mla) {
 #if 0
@@ -1276,11 +1292,11 @@ ggml_tensor * llm_graph_context::build_attn_mha(
             cur = ggml_permute(ctx0, cur, 0, 2, 1, 3);
             cur = ggml_mul_mat(ctx0, v_mla, cur);
             cur = ggml_permute(ctx0, cur, 0, 2, 1, 3);
-            cur = ggml_cont(ctx0, cur); // Needed because ggml_reshape_2d expects contiguous inputs.
+            cur = ggml_cont(ctx0, cur);  // Needed because ggml_reshape_2d expects contiguous inputs.
 #endif
         }
 
-        cur = ggml_reshape_2d(ctx0, cur, cur->ne[0]*cur->ne[1], cur->ne[2]*cur->ne[3]);
+        cur = ggml_reshape_2d(ctx0, cur, cur->ne[0] * cur->ne[1], cur->ne[2] * cur->ne[3]);
     } else {
         ggml_tensor * kq = ggml_mul_mat(ctx0, k, q);
 
@@ -1295,13 +1311,13 @@ ggml_tensor * llm_graph_context::build_attn_mha(
             // kq = 30 * tanh(kq / 30)
             // before the softmax below
 
-            kq = ggml_tanh(ctx0, ggml_scale(ctx0, kq, 0.08838834764831845f/30.0f));
+            kq = ggml_tanh(ctx0, ggml_scale(ctx0, kq, 0.08838834764831845f / 30.0f));
             kq = ggml_scale(ctx0, kq, 30);
         }
 
         if (hparams.attn_soft_cap) {
             kq = ggml_scale(ctx0, kq, 1.0f / hparams.f_attn_logit_softcapping);
-            kq = ggml_tanh (ctx0, kq);
+            kq = ggml_tanh(ctx0, kq);
             kq = ggml_scale(ctx0, kq, hparams.f_attn_logit_softcapping);
         }
 
@@ -1327,7 +1343,7 @@ ggml_tensor * llm_graph_context::build_attn_mha(
         cur = ggml_permute(ctx0, kqv, 0, 2, 1, 3);
 
         // recombine streams
-        cur = ggml_cont_2d(ctx0, cur, cur->ne[0]*cur->ne[1], cur->ne[2]*cur->ne[3]);
+        cur = ggml_cont_2d(ctx0, cur, cur->ne[0] * cur->ne[1], cur->ne[2] * cur->ne[3]);
 
         if (!cparams.offload_kqv) {
             // all nodes between the KV store and the attention output are run on the CPU
@@ -1352,17 +1368,16 @@ llm_graph_input_attn_no_cache * llm_graph_context::build_attn_inp_no_cache() con
     return (llm_graph_input_attn_no_cache *) res->add_input(std::move(inp));
 }
 
-ggml_tensor * llm_graph_context::build_attn(
-        llm_graph_input_attn_no_cache * inp,
-        ggml_tensor * wo,
-        ggml_tensor * wo_b,
-        ggml_tensor * q_cur,
-        ggml_tensor * k_cur,
-        ggml_tensor * v_cur,
-        ggml_tensor * kq_b,
-        ggml_tensor * v_mla,
-            float     kq_scale,
-            int       il) const {
+ggml_tensor * llm_graph_context::build_attn(llm_graph_input_attn_no_cache * inp,
+                                            ggml_tensor *                   wo,
+                                            ggml_tensor *                   wo_b,
+                                            ggml_tensor *                   q_cur,
+                                            ggml_tensor *                   k_cur,
+                                            ggml_tensor *                   v_cur,
+                                            ggml_tensor *                   kq_b,
+                                            ggml_tensor *                   v_mla,
+                                            float                           kq_scale,
+                                            int                             il) const {
     GGML_UNUSED(n_tokens);
 
     // these nodes are added to the graph together so that they are not reordered
@@ -1400,12 +1415,11 @@ ggml_tensor * llm_graph_context::build_attn(
 }
 
 static std::unique_ptr<llm_graph_input_attn_kv_unified> build_attn_inp_kv_unified_impl(
-           ggml_context * ctx0,
-     const llama_ubatch & ubatch,
-    const llama_hparams & hparams,
-    const llama_cparams & cparams,
+    ggml_context *                         ctx0,
+    const llama_ubatch &                   ubatch,
+    const llama_hparams &                  hparams,
+    const llama_cparams &                  cparams,
     const llama_kv_cache_unified_context * mctx_cur) {
-
     auto inp = std::make_unique<llm_graph_input_attn_kv_unified>(hparams, cparams, mctx_cur);
 
     {
@@ -1418,10 +1432,12 @@ static std::unique_ptr<llm_graph_input_attn_kv_unified> build_attn_inp_kv_unifie
         inp->self_k_idxs = mctx_cur->build_input_k_idxs(ctx0, ubatch);
         inp->self_v_idxs = mctx_cur->build_input_v_idxs(ctx0, ubatch);
 
-        inp->self_kq_mask = ggml_new_tensor_4d(ctx0, GGML_TYPE_F32, n_kv, GGML_PAD(n_tokens/n_stream, GGML_KQ_MASK_PAD), 1, n_stream);
+        inp->self_kq_mask =
+            ggml_new_tensor_4d(ctx0, GGML_TYPE_F32, n_kv, GGML_PAD(n_tokens / n_stream, GGML_KQ_MASK_PAD), 1, n_stream);
         ggml_set_input(inp->self_kq_mask);
 
-        inp->self_kq_mask_cnv = cparams.flash_attn ? ggml_cast(ctx0, inp->self_kq_mask, GGML_TYPE_F16) : inp->self_kq_mask;
+        inp->self_kq_mask_cnv =
+            cparams.flash_attn ? ggml_cast(ctx0, inp->self_kq_mask, GGML_TYPE_F16) : inp->self_kq_mask;
     }
 
     return inp;
@@ -1435,17 +1451,16 @@ llm_graph_input_attn_kv_unified * llm_graph_context::build_attn_inp_kv_unified()
     return (llm_graph_input_attn_kv_unified *) res->add_input(std::move(inp));
 }
 
-ggml_tensor * llm_graph_context::build_attn(
-        llm_graph_input_attn_kv_unified * inp,
-        ggml_tensor * wo,
-        ggml_tensor * wo_b,
-        ggml_tensor * q_cur,
-        ggml_tensor * k_cur,
-        ggml_tensor * v_cur,
-        ggml_tensor * kq_b,
-        ggml_tensor * v_mla,
-            float     kq_scale,
-            int       il) const {
+ggml_tensor * llm_graph_context::build_attn(llm_graph_input_attn_kv_unified * inp,
+                                            ggml_tensor *                     wo,
+                                            ggml_tensor *                     wo_b,
+                                            ggml_tensor *                     q_cur,
+                                            ggml_tensor *                     k_cur,
+                                            ggml_tensor *                     v_cur,
+                                            ggml_tensor *                     kq_b,
+                                            ggml_tensor *                     v_mla,
+                                            float                             kq_scale,
+                                            int                               il) const {
     // these nodes are added to the graph together so that they are not reordered
     // by doing so, the number of splits in the graph is reduced
     ggml_build_forward_expand(gf, q_cur);
@@ -1487,43 +1502,30 @@ ggml_tensor * llm_graph_context::build_attn(
     return cur;
 }
 
-ggml_tensor * llm_graph_context::build_attn(
-        llm_graph_input_attn_kv_unified_iswa * inp,
-        ggml_tensor * wo,
-        ggml_tensor * wo_b,
-        ggml_tensor * q_cur,
-        ggml_tensor * k_cur,
-        ggml_tensor * v_cur,
-        ggml_tensor * kq_b,
-        ggml_tensor * v_mla,
-            float     kq_scale,
-            int       il) const {
-    return build_attn_with_sinks(
-            inp,
-            wo,
-            wo_b,
-            q_cur,
-            k_cur,
-            v_cur,
-            kq_b,
-            v_mla,
-            nullptr,
-            kq_scale,
-            il);
-}
-
-ggml_tensor * llm_graph_context::build_attn_with_sinks(
-        llm_graph_input_attn_kv_unified_iswa * inp,
-        ggml_tensor * wo,
-        ggml_tensor * wo_b,
-        ggml_tensor * q_cur,
-        ggml_tensor * k_cur,
-        ggml_tensor * v_cur,
-        ggml_tensor * kq_b,
-        ggml_tensor * v_mla,
-        ggml_tensor * sinks,
-            float     kq_scale,
-            int       il) const {
+ggml_tensor * llm_graph_context::build_attn(llm_graph_input_attn_kv_unified_iswa * inp,
+                                            ggml_tensor *                          wo,
+                                            ggml_tensor *                          wo_b,
+                                            ggml_tensor *                          q_cur,
+                                            ggml_tensor *                          k_cur,
+                                            ggml_tensor *                          v_cur,
+                                            ggml_tensor *                          kq_b,
+                                            ggml_tensor *                          v_mla,
+                                            float                                  kq_scale,
+                                            int                                    il) const {
+    return build_attn_with_sinks(inp, wo, wo_b, q_cur, k_cur, v_cur, kq_b, v_mla, nullptr, kq_scale, il);
+}
+
+ggml_tensor * llm_graph_context::build_attn_with_sinks(llm_graph_input_attn_kv_unified_iswa * inp,
+                                                       ggml_tensor *                          wo,
+                                                       ggml_tensor *                          wo_b,
+                                                       ggml_tensor *                          q_cur,
+                                                       ggml_tensor *                          k_cur,
+                                                       ggml_tensor *                          v_cur,
+                                                       ggml_tensor *                          kq_b,
+                                                       ggml_tensor *                          v_mla,
+                                                       ggml_tensor *                          sinks,
+                                                       float                                  kq_scale,
+                                                       int                                    il) const {
     // these nodes are added to the graph together so that they are not reordered
     // by doing so, the number of splits in the graph is reduced
     ggml_build_forward_expand(gf, q_cur);
@@ -1587,22 +1589,22 @@ llm_graph_input_attn_cross * llm_graph_context::build_attn_inp_cross() const {
     inp->cross_kq_mask = ggml_new_tensor_4d(ctx0, GGML_TYPE_F32, n_enc, GGML_PAD(n_tokens, GGML_KQ_MASK_PAD), 1, 1);
     ggml_set_input(inp->cross_kq_mask);
 
-    inp->cross_kq_mask_cnv = cparams.flash_attn ? ggml_cast(ctx0, inp->cross_kq_mask, GGML_TYPE_F16) : inp->cross_kq_mask;
+    inp->cross_kq_mask_cnv =
+        cparams.flash_attn ? ggml_cast(ctx0, inp->cross_kq_mask, GGML_TYPE_F16) : inp->cross_kq_mask;
 
     return (llm_graph_input_attn_cross *) res->add_input(std::move(inp));
 }
 
-ggml_tensor * llm_graph_context::build_attn(
-        llm_graph_input_attn_cross * inp,
-        ggml_tensor * wo,
-        ggml_tensor * wo_b,
-        ggml_tensor * q_cur,
-        ggml_tensor * k_cur,
-        ggml_tensor * v_cur,
-        ggml_tensor * kq_b,
-        ggml_tensor * v_mla,
-            float     kq_scale,
-            int       il) const {
+ggml_tensor * llm_graph_context::build_attn(llm_graph_input_attn_cross * inp,
+                                            ggml_tensor *                wo,
+                                            ggml_tensor *                wo_b,
+                                            ggml_tensor *                q_cur,
+                                            ggml_tensor *                k_cur,
+                                            ggml_tensor *                v_cur,
+                                            ggml_tensor *                kq_b,
+                                            ggml_tensor *                v_mla,
+                                            float                        kq_scale,
+                                            int                          il) const {
     // these nodes are added to the graph together so that they are not reordered
     // by doing so, the number of splits in the graph is reduced
     ggml_build_forward_expand(gf, q_cur);
@@ -1649,10 +1651,12 @@ llm_graph_input_attn_kv_unified_iswa * llm_graph_context::build_attn_inp_kv_unif
         inp->self_k_idxs = mctx_cur->get_base()->build_input_k_idxs(ctx0, ubatch);
         inp->self_v_idxs = mctx_cur->get_base()->build_input_v_idxs(ctx0, ubatch);
 
-        inp->self_kq_mask = ggml_new_tensor_4d(ctx0, GGML_TYPE_F32, n_kv, GGML_PAD(n_tokens/n_stream, GGML_KQ_MASK_PAD), 1, n_stream);
+        inp->self_kq_mask =
+            ggml_new_tensor_4d(ctx0, GGML_TYPE_F32, n_kv, GGML_PAD(n_tokens / n_stream, GGML_KQ_MASK_PAD), 1, n_stream);
         ggml_set_input(inp->self_kq_mask);
 
-        inp->self_kq_mask_cnv = cparams.flash_attn ? ggml_cast(ctx0, inp->self_kq_mask, GGML_TYPE_F16) : inp->self_kq_mask;
+        inp->self_kq_mask_cnv =
+            cparams.flash_attn ? ggml_cast(ctx0, inp->self_kq_mask, GGML_TYPE_F16) : inp->self_kq_mask;
     }
 
     {
@@ -1663,32 +1667,33 @@ llm_graph_input_attn_kv_unified_iswa * llm_graph_context::build_attn_inp_kv_unif
         inp->self_k_idxs_swa = mctx_cur->get_swa()->build_input_k_idxs(ctx0, ubatch);
         inp->self_v_idxs_swa = mctx_cur->get_swa()->build_input_v_idxs(ctx0, ubatch);
 
-        inp->self_kq_mask_swa = ggml_new_tensor_4d(ctx0, GGML_TYPE_F32, n_kv, GGML_PAD(n_tokens/n_stream, GGML_KQ_MASK_PAD), 1, n_stream);
+        inp->self_kq_mask_swa =
+            ggml_new_tensor_4d(ctx0, GGML_TYPE_F32, n_kv, GGML_PAD(n_tokens / n_stream, GGML_KQ_MASK_PAD), 1, n_stream);
         ggml_set_input(inp->self_kq_mask_swa);
 
-        inp->self_kq_mask_swa_cnv = cparams.flash_attn ? ggml_cast(ctx0, inp->self_kq_mask_swa, GGML_TYPE_F16) : inp->self_kq_mask_swa;
+        inp->self_kq_mask_swa_cnv =
+            cparams.flash_attn ? ggml_cast(ctx0, inp->self_kq_mask_swa, GGML_TYPE_F16) : inp->self_kq_mask_swa;
     }
 
     return (llm_graph_input_attn_kv_unified_iswa *) res->add_input(std::move(inp));
 }
 
-ggml_tensor * llm_graph_context::build_rs(
-        ggml_tensor * s,
-        ggml_tensor * state_copy_main,
-        ggml_tensor * state_copy_extra,
-            int32_t   state_size,
-            int32_t   n_seqs,
-           uint32_t   n_rs,
-           uint32_t   rs_head,
-           uint32_t   rs_size,
-            int32_t   rs_zero,
-        const llm_graph_get_rows_fn & get_state_rows) const {
-
+ggml_tensor * llm_graph_context::build_rs(ggml_tensor *                 s,
+                                          ggml_tensor *                 state_copy_main,
+                                          ggml_tensor *                 state_copy_extra,
+                                          int32_t                       state_size,
+                                          int32_t                       n_seqs,
+                                          uint32_t                      n_rs,
+                                          uint32_t                      rs_head,
+                                          uint32_t                      rs_size,
+                                          int32_t                       rs_zero,
+                                          const llm_graph_get_rows_fn & get_state_rows) const {
     ggml_tensor * states = ggml_reshape_2d(ctx0, s, state_size, rs_size);
 
     // Clear a single state which will then be copied to the other cleared states.
     // Note that this is a no-op when the view is zero-sized.
-    ggml_tensor * state_zero = ggml_view_1d(ctx0, states, state_size*(rs_zero >= 0), rs_zero*states->nb[1]*(rs_zero >= 0));
+    ggml_tensor * state_zero =
+        ggml_view_1d(ctx0, states, state_size * (rs_zero >= 0), rs_zero * states->nb[1] * (rs_zero >= 0));
     ggml_build_forward_expand(gf, ggml_scale_inplace(ctx0, state_zero, 0));
 
     // copy states
@@ -1699,19 +1704,19 @@ ggml_tensor * llm_graph_context::build_rs(
 
     // copy extra states which won't be changed further (between n_seqs and n_rs)
     ggml_tensor * states_extra = ggml_get_rows(ctx0, states, state_copy_extra);
-    ggml_build_forward_expand(gf,
+    ggml_build_forward_expand(
+        gf,
         ggml_cpy(ctx0,
-            states_extra,
-            ggml_view_1d(ctx0, s, state_size*(n_rs - n_seqs), (rs_head + n_seqs)*state_size*ggml_element_size(s))));
+                 states_extra,
+                 ggml_view_1d(
+                     ctx0, s, state_size * (n_rs - n_seqs), (rs_head + n_seqs) * state_size * ggml_element_size(s))));
 
     return output_states;
 }
 
-static std::unique_ptr<llm_graph_input_rs> build_rs_inp_impl(
-           ggml_context * ctx0,
-     const llama_ubatch & ubatch,
-    const llama_memory_recurrent_context * mctx_cur) {
-
+static std::unique_ptr<llm_graph_input_rs> build_rs_inp_impl(ggml_context *                         ctx0,
+                                                             const llama_ubatch &                   ubatch,
+                                                             const llama_memory_recurrent_context * mctx_cur) {
     auto inp = std::make_unique<llm_graph_input_rs>(mctx_cur);
 
     const int64_t n_rs   = mctx_cur->get_n_rs();
@@ -1734,58 +1739,61 @@ llm_graph_input_rs * llm_graph_context::build_rs_inp() const {
     return (llm_graph_input_rs *) res->add_input(std::move(inp));
 }
 
-ggml_tensor * llm_graph_context::build_rs(
-        llm_graph_input_rs * inp,
-        ggml_tensor * s,
-            int32_t   state_size,
-            int32_t   n_seqs,
-        const llm_graph_get_rows_fn & get_state_rows) const {
+ggml_tensor * llm_graph_context::build_rs(llm_graph_input_rs *          inp,
+                                          ggml_tensor *                 s,
+                                          int32_t                       state_size,
+                                          int32_t                       n_seqs,
+                                          const llm_graph_get_rows_fn & get_state_rows) const {
     const auto * kv_state = inp->mctx;
 
-    return build_rs(s, inp->s_copy_main, inp->s_copy_extra, state_size, n_seqs,
-                    kv_state->get_n_rs(), kv_state->get_head(), kv_state->get_size(), kv_state->get_rs_z(),
+    return build_rs(s,
+                    inp->s_copy_main,
+                    inp->s_copy_extra,
+                    state_size,
+                    n_seqs,
+                    kv_state->get_n_rs(),
+                    kv_state->get_head(),
+                    kv_state->get_size(),
+                    kv_state->get_rs_z(),
                     get_state_rows);
 }
 
-ggml_tensor * llm_graph_context::build_rwkv_token_shift_load(
-    llm_graph_input_rs * inp,
-    const llama_ubatch & ubatch,
-                   int   il) const {
+ggml_tensor * llm_graph_context::build_rwkv_token_shift_load(llm_graph_input_rs * inp,
+                                                             const llama_ubatch & ubatch,
+                                                             int                  il) const {
     const auto * mctx_cur = static_cast<const llama_memory_recurrent_context *>(mctx);
 
     const auto token_shift_count = hparams.token_shift_count;
 
-    const int64_t n_seqs  = ubatch.n_seqs;
+    const int64_t n_seqs = ubatch.n_seqs;
 
     ggml_tensor * token_shift_all = mctx_cur->get_r_l(il);
 
-    ggml_tensor * token_shift = build_rs(
-            inp, token_shift_all,
-            hparams.n_embd_r(), n_seqs);
+    ggml_tensor * token_shift = build_rs(inp, token_shift_all, hparams.n_embd_r(), n_seqs);
 
     token_shift = ggml_reshape_3d(ctx0, token_shift, hparams.n_embd, token_shift_count, n_seqs);
 
     return token_shift;
 }
 
-ggml_tensor * llm_graph_context::build_rwkv_token_shift_store(
-         ggml_tensor * token_shift,
-  const llama_ubatch & ubatch,
-                 int   il) const {
+ggml_tensor * llm_graph_context::build_rwkv_token_shift_store(ggml_tensor *        token_shift,
+                                                              const llama_ubatch & ubatch,
+                                                              int                  il) const {
     const auto * mctx_cur = static_cast<const llama_memory_recurrent_context *>(mctx);
 
     const auto token_shift_count = hparams.token_shift_count;
-    const auto n_embd = hparams.n_embd;
+    const auto n_embd            = hparams.n_embd;
 
     const int64_t n_seqs = ubatch.n_seqs;
 
     const auto kv_head = mctx_cur->get_head();
 
-    return ggml_cpy(
-        ctx0,
-        ggml_view_1d(ctx0, token_shift, n_embd * n_seqs * token_shift_count, 0),
-        ggml_view_1d(ctx0, mctx_cur->get_r_l(il), hparams.n_embd_r()*n_seqs, hparams.n_embd_r()*kv_head*ggml_element_size(mctx_cur->get_r_l(il)))
-    );
+    return ggml_cpy(ctx0,
+                    ggml_view_1d(ctx0, token_shift, n_embd * n_seqs * token_shift_count, 0),
+                    ggml_view_1d(ctx0,
+                                 mctx_cur->get_r_l(il),
+                                 hparams.n_embd_r() * n_seqs,
+                                 hparams.n_embd_r() * kv_head * ggml_element_size(mctx_cur->get_r_l(il))));
 }
 
 llm_graph_input_mem_hybrid * llm_graph_context::build_inp_mem_hybrid() const {
@@ -1799,11 +1807,10 @@ llm_graph_input_mem_hybrid * llm_graph_context::build_inp_mem_hybrid() const {
     return (llm_graph_input_mem_hybrid *) res->add_input(std::move(inp));
 }
 
-void llm_graph_context::build_pooling(
-        ggml_tensor * cls,
-        ggml_tensor * cls_b,
-        ggml_tensor * cls_out,
-        ggml_tensor * cls_out_b) const {
+void llm_graph_context::build_pooling(ggml_tensor * cls,
+                                      ggml_tensor * cls_b,
+                                      ggml_tensor * cls_out,
+                                      ggml_tensor * cls_out_b) const {
     if (!cparams.embeddings) {
         return;
     }
@@ -1828,22 +1835,25 @@ void llm_graph_context::build_pooling(
         case LLAMA_POOLING_TYPE_NONE:
             {
                 cur = inp;
-            } break;
+            }
+            break;
         case LLAMA_POOLING_TYPE_MEAN:
             {
                 ggml_tensor * inp_mean = build_inp_mean();
-                cur = ggml_mul_mat(ctx0, ggml_cont(ctx0, ggml_transpose(ctx0, inp)), inp_mean);
-            } break;
+                cur                    = ggml_mul_mat(ctx0, ggml_cont(ctx0, ggml_transpose(ctx0, inp)), inp_mean);
+            }
+            break;
         case LLAMA_POOLING_TYPE_CLS:
         case LLAMA_POOLING_TYPE_LAST:
             {
                 ggml_tensor * inp_cls = build_inp_cls();
-                cur = ggml_get_rows(ctx0, inp, inp_cls);
-            } break;
+                cur                   = ggml_get_rows(ctx0, inp, inp_cls);
+            }
+            break;
         case LLAMA_POOLING_TYPE_RANK:
             {
                 ggml_tensor * inp_cls = build_inp_cls();
-                inp = ggml_get_rows(ctx0, inp, inp_cls);
+                inp                   = ggml_get_rows(ctx0, inp, inp_cls);
 
                 if (cls) {
                     // classification head
@@ -1872,7 +1882,8 @@ void llm_graph_context::build_pooling(
                 } else {
                     GGML_ABORT("RANK pooling requires either cls+cls_b or cls_out+cls_out_b");
                 }
-            } break;
+            }
+            break;
         default:
             {
                 GGML_ABORT("unknown pooling type");
@@ -1896,7 +1907,7 @@ int32_t llama_relative_position_bucket(llama_pos x, llama_pos y, uint64_t n_buck
     const int64_t max_exact = n_buckets >> 1;
 
     int32_t relative_position = x - y;
-    int32_t relative_bucket = 0;
+    int32_t relative_bucket   = 0;
 
     if (bidirectional) {
         relative_bucket += (relative_position > 0) * n_buckets;
@@ -1905,7 +1916,9 @@ int32_t llama_relative_position_bucket(llama_pos x, llama_pos y, uint64_t n_buck
         relative_position = -std::min<int32_t>(relative_position, 0);
     }
 
-    int32_t relative_position_if_large = floorf(max_exact + logf(1.0 * relative_position / max_exact) * (n_buckets - max_exact) / log(1.0 * max_distance / max_exact));
+    int32_t relative_position_if_large =
+        floorf(max_exact + logf(1.0 * relative_position / max_exact) * (n_buckets - max_exact) /
+                               log(1.0 * max_distance / max_exact));
     relative_position_if_large = std::min<int32_t>(relative_position_if_large, n_buckets - 1);
     relative_bucket += (relative_position < max_exact ? relative_position : relative_position_if_large);
 
